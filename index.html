<!DOCTYPE html>



  


<html class="theme-next mist use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT" />










<meta property="og:type" content="website">
<meta property="og:title" content="COOLA-LAB">
<meta property="og:url" content="http://yoursite.com/index.html">
<meta property="og:site_name" content="COOLA-LAB">
<meta property="article:author" content="COOLA-LAB">
<meta name="twitter:card" content="summary">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/"/>





  <title>COOLA-LAB</title>
  








<meta name="generator" content="Hexo 4.2.1"></head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">COOLA-LAB</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/11/11/RouteNet-Leveraging-Graph-Neural-Networks-for-Network-Modeling-and-Optimization-in-SDN/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="COOLA-LAB">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="COOLA-LAB">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/11/11/RouteNet-Leveraging-Graph-Neural-Networks-for-Network-Modeling-and-Optimization-in-SDN/" itemprop="url">RouteNet: Leveraging Graph Neural Networks for Network Modeling and Optimization in SDN </a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-11-11T08:43:27+08:00">
                2020-11-11
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%AF%8F%E5%91%A8%E4%B8%80%E4%BC%9A/" itemprop="url" rel="index">
                    <span itemprop="name">每周一会</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>这篇论文提出了一种叫做RouteNet的方法，这是一种基于MPNN的新型网络模型，用于预测某种场景下的网络的特征矩阵，该模型能够了解拓扑，路由和输入流量之间的复杂关系，从而可以准确估算每个源/目标每个数据包的KPI指标。</p>
<h2 id="1-问题描述"><a href="#1-问题描述" class="headerlink" title="1. 问题描述"></a>1. 问题描述</h2><h3 id="一些相关概念"><a href="#一些相关概念" class="headerlink" title="一些相关概念"></a>一些相关概念</h3><ul>
<li>KPI—(Key performance Indicator)<ul>
<li>时延（packet发送和接受的时间差），抖动（相邻packet时延的差值），丢包率（一段时间内丢失的packet比例）等一些描述网络性能的指标，具体可以由一个邻接矩阵矩阵来表示每个节点对之间的一段时间内的平均表现。</li>
</ul>
</li>
<li>网络优化—(Network optimization)<ul>
<li>进行一系列操作使得网络的性能提升</li>
<li>网络优化可以分为两部分循环迭代：1）通过网络模型获取量化的网络性能指标，2）将量化的指标输入优化器获得结果</li>
</ul>
</li>
<li>网络模型—(Network modeling)<ul>
<li>网络优化首先要能度量网络的性能（“<em>we can only optimize what we can model.</em> ”），网络模型就是负责量化网络的目前性能</li>
<li>主流的做法有两个：数学模型（主要是基于排队论的基础—特点是精度低但是速度快）、仿真器（能获得精确的结果，但是仿真速度比较慢）</li>
</ul>
</li>
</ul>
<p>在SDN网络结构中<img src="/2020/11/11/RouteNet-Leveraging-Graph-Neural-Networks-for-Network-Modeling-and-Optimization-in-SDN/image-20201110220025370.png" alt="image-20201110220025370">，如图所示可以实时获取全局的网络信息，从而进行优化，以达到相应的优化目标，左边部分为数据面板，右边部分为控制面板，整个网络优化的过程就可以部署在控制面板中。</p>
<h3 id="本文主要解决问题"><a href="#本文主要解决问题" class="headerlink" title="本文主要解决问题"></a>本文主要解决问题</h3><p>文章主要是利用MPNN的思想建立了一个监督学习的神经网络用来预测网络的KPI指标，监督的样本来自仿真器的Ground-Truth，最后训练出来的模型具有应对类似拓扑的泛化能力以及实验证明可靠的精度。</p>
<p>换言之，主要就是用AI代替了传统网络优化过程中的网络KPI量化这一步骤。</p>
<h2 id="2-主要思想"><a href="#2-主要思想" class="headerlink" title="2. 主要思想"></a>2. 主要思想</h2><h3 id="MPNN-Message-Passing-Neural-Network-—通用GCN框架"><a href="#MPNN-Message-Passing-Neural-Network-—通用GCN框架" class="headerlink" title="MPNN(Message-Passing Neural Network)—通用GCN框架"></a>MPNN(Message-Passing Neural Network)—通用GCN框架</h3><p>文章作者表示，方法的主要思想起源于MPNN（Gilmer J, Schoenholz S S, Riley P F, et al. Neural message passing for quantum chemistry[J]. arXiv preprint arXiv:1704.01212, 2017.），这篇文章里主要概括了之前的主要GCN模型，并抽象出一个普适的GCN框架，这个框架将GCN网络分为三部分：</p>
<ol>
<li>Message Passing</li>
<li>State Update</li>
<li>Readout</li>
</ol>
<p>这边结合之前张凯学长的一篇<a href="https://coola-lab.github.io/2020/09/20/Inductive-Representation-Learning-on-Large-Graphs/" target="_blank" rel="noopener">组会报告</a>来解释这个框架，这篇报告的文章提出了一个GraphSAGE的方法，算法过程如下：<img src="/2020/11/11/RouteNet-Leveraging-Graph-Neural-Networks-for-Network-Modeling-and-Optimization-in-SDN/image-20201110224404481.png" alt></p>
<p>算法过程中标出的部分即为三个MPNN的三个步骤。</p>
<p>GCN的贡献就在于能提取出传统网络的拓扑信息，然后进行接下来的分类回归等操作。</p>
<h3 id="RouteNet"><a href="#RouteNet" class="headerlink" title="RouteNet"></a>RouteNet</h3><p>一个packet传输经过若干条链路，所有链路的集合称之为路径，作者的模型基于以下的原则：</p>
<ol>
<li>每个路径的状态取决于所有链路的状态</li>
<li>一个链路的状态取决于所有经过这条链路的路径的状态</li>
</ol>
<h2 id="3-方法"><a href="#3-方法" class="headerlink" title="3. 方法"></a>3. 方法</h2><p>算法过程如下图所示：<img src="/2020/11/11/RouteNet-Leveraging-Graph-Neural-Networks-for-Network-Modeling-and-Optimization-in-SDN/image-20201110231404662.png" alt="image-20201110231404662"></p>
<p>首先初始化路径和各个链路的初始状态，接着对每个链路和路径进行Message Passing 操作，最后再进行readout操作，通过反向传播进行学习。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/10/26/SiamFC-Towards-Robust-and-Accurate-Visual-Tracking-with-Target-Estimation-Guidelines/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="COOLA-LAB">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="COOLA-LAB">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/10/26/SiamFC-Towards-Robust-and-Accurate-Visual-Tracking-with-Target-Estimation-Guidelines/" itemprop="url">SiamFC++: Towards Robust and Accurate Visual Tracking with Target Estimation Guidelines</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-10-26T20:46:34+08:00">
                2020-10-26
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%AF%8F%E5%91%A8%E4%B8%80%E4%BC%9A/" itemprop="url" rel="index">
                    <span itemprop="name">每周一会</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>本文通过分析和比较各类跟踪器在目标状态估计任务上的表现，做出两项主要贡献：提出一套高性能通用跟踪器的设计要点；根据要点设计出SiamFC++跟踪器，同时证明设计要点的有效性。</p>
<h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h2><p>跟踪任务可以被分解为分类任务和状态估计任务的组合：第一个任务的目的是通过分类指出目标的粗略位置，第二个任务则旨在估计精确的目标状态。</p>
<p>对于第二项任务，过去的跟踪器采取以下三种方式：以DCF和SiamFC为代表，采用粗暴的多尺度测试；以ATOM为代表，用梯度递增迭代地调整最初的目标框；以及SiamRPN家族——引入RPN思想。在对以上三种类型的方式进行分析和对比以后，本文总结出四点通用跟踪器的设计要点：</p>
<p>G1：Decomposition of classification and state estimation，分开进行分类和状态估计。</p>
<p>G2：Non-ambiguous scoring，无歧义分数，实际上是具有准确对应关系的分类置信分数。</p>
<p>G3：Prior knowledge-free，无先验知识。目标跟踪任务不应当拥有目标的尺度、大小等先验知识。</p>
<p>G4：Estimation quality assessment，对状态估计质量的独立评估。</p>
<h2 id="2-Main-Idea"><a href="#2-Main-Idea" class="headerlink" title="2. Main Idea"></a>2. Main Idea</h2><p>本文首先遵循G1原则做两个分支。</p>
<p><img src="/2020/10/26/SiamFC-Towards-Robust-and-Accurate-Visual-Tracking-with-Target-Estimation-Guidelines/1.jpg" style="zoom:70%;"></p>
<p>分类部分与siamFC基本一致，采取位置作为训练样本——假如响应图中某个点对应的原图位置位于真实的目标范围中，则这个位置属于正样本。</p>
<p>对于回归分支，其设计思路也是针对每一个位置做回归——最后一层预测特征图上的每个位置$(x,y)$对于的输入图像位置$\left(\left\lfloor\frac{s}{2}\right\rfloor+ xs,\left\lfloor\frac{s}{2}\right\rfloor+ ys\right)$到地面真实边界框上下左右四个边的距离定义为一个4维的向量：</p>
<script type="math/tex; mode=display">
t 
∗
 =(l 
∗
 ,t 
∗
 ,r 
∗
 ,b 
∗
 )</script><p>则对该位置的回归任务可以形式化为：</p>
<script type="math/tex; mode=display">
l^{*}=\left(\left\lfloor\frac{s}{2}\right\rfloor+ xs\right)-x_0</script><script type="math/tex; mode=display">
t^{*}=\left(\left\lfloor\frac{s}{2}\right\rfloor+ ys\right)-y_0</script><script type="math/tex; mode=display">
r^{*}=x_1-\left(\left\lfloor\frac{s}{2}\right\rfloor+ xs\right)</script><script type="math/tex; mode=display">
b^{*}=y_1-\left(\left\lfloor\frac{s}{2}\right\rfloor+ ys\right)</script><p>其中$ (x_0,y_0)$和$ (x_1,y_1)$表示与$(x,y)对应的真实边界框$B^{*}$的左上角和右下角。</p>
<p>由于分类分数和回归操作都是针对“位置”这个概念的，也就是操作的区域都是基于该位置的周围一片区域，而不是与人为设定的anchor对应，所以满足非歧义性（G2）。加上没有预定义anchor，所以也没有先验知识的应用，符合（G3）。</p>
<p>接下来完成G4——对估计质量的独立评价：本文通过添加$1 × 1$卷积层来添加简单但有效的质量评估分支,该层输出反映子窗口中心周围的输入像素在跟踪问题中重要性高于其余部分的优先空间得分（Prior Spatial Score，PSS）：</p>
<script type="math/tex; mode=display">
PSS^{*}=\sqrt{\frac {min(l^{*},r^{*})}{max(l^{*},r^{*})}× \frac {min(t^{*},b^{*})}{max(t^{*},b^{*})}}</script><p>通过将PSS与相应的预测分类分数相乘来产生用于最终框选择的分数。这样，远离物体中心的点的权重就会大大下降，从而提高了跟踪精度。</p>
<h2 id="3-Experiments"><a href="#3-Experiments" class="headerlink" title="3. Experiments"></a>3. Experiments</h2><h3 id="3-1-From-SiamFC-towards-SiamFC"><a href="#3-1-From-SiamFC-towards-SiamFC" class="headerlink" title="3.1 From SiamFC towards SiamFC++"></a>3.1 From SiamFC towards SiamFC++</h3><p><img src="/2020/10/26/SiamFC-Towards-Robust-and-Accurate-Visual-Tracking-with-Target-Estimation-Guidelines/2.jpg"></p>
<h3 id="3-2-Results-on-Several-Benchmarks"><a href="#3-2-Results-on-Several-Benchmarks" class="headerlink" title="3.2 Results on Several Benchmarks"></a>3.2 Results on Several Benchmarks</h3><p><img src="/2020/10/26/SiamFC-Towards-Robust-and-Accurate-Visual-Tracking-with-Target-Estimation-Guidelines/3.jpg" style="zoom:80%;"></p>
<h3 id="3-3-Comparison-with-Trackers-that-Do-not-Apply-Our-Guidelines"><a href="#3-3-Comparison-with-Trackers-that-Do-not-Apply-Our-Guidelines" class="headerlink" title="3.3 Comparison with Trackers that Do not Apply Our Guidelines"></a>3.3 Comparison with Trackers that Do not Apply Our Guidelines</h3><p><img src="/2020/10/26/SiamFC-Towards-Robust-and-Accurate-Visual-Tracking-with-Target-Estimation-Guidelines/4.jpg" style="zoom:80%;"></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/09/22/Self-Supervised-Learning-of-Depth-and-Motion-Under-Photometric-Inconsistency/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="COOLA-LAB">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="COOLA-LAB">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/09/22/Self-Supervised-Learning-of-Depth-and-Motion-Under-Photometric-Inconsistency/" itemprop="url">Self-Supervised Learning of Depth and Motion Under Photometric Inconsistency</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-09-22T19:00:00+08:00">
                2020-09-22
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%AF%8F%E5%91%A8%E4%B8%80%E4%BC%9A/" itemprop="url" rel="index">
                    <span itemprop="name">每周一会</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>在自监督单目深度估计任务中，文章提出了一种改进的框架，通过增加几何约束和尺度一致性约束，解决动态场景下光度不一致等问题，并增强时间图像序列中的尺度一致性，提升了单目深度和相机自运动估计的结果。</p>
<h2 id="1-Previous-work"><a href="#1-Previous-work" class="headerlink" title="1. Previous work"></a>1. Previous work</h2><p>单目深度估计是指通过机器人的单一摄像头获得的一帧或多帧图像来获取深度信息和机器人姿态的方法。在即使定位与地图构建（SLAM）和视觉里程计（VO）中广泛应用。传统的深度估计方法根据图像像素的特征点进行匹配，受噪声影响误差较大，2017年，Google在CVPR上发表论文Unsupervised Learning of Depth and Ego-motion from Video，提出了自监督学习框架，可以对连续的视频序列做单目深度和相机运动的估计任务。</p>
<script type="math/tex; mode=display">
p_s \sim K_s[R_{t\to s}|t_{t\to s}]D_t(p_t)K^{-1}_tp_t</script><p><img src="/2020/09/22/Self-Supervised-Learning-of-Depth-and-Motion-Under-Photometric-Inconsistency/1.png" alt="1" style="zoom:50%;"></p>
<p><img src="/2020/09/22/Self-Supervised-Learning-of-Depth-and-Motion-Under-Photometric-Inconsistency/2.png" alt="2" style="zoom:50%;"></p>
<p>这样的估计方法使用光度误差做自监督，存在一定的局限性：</p>
<ul>
<li>场景中存在移动物体、非漫反射表面和遮挡时，光度误差损失函数不适用</li>
<li>在单目场景下，单张物体深度估计结果是基于单张图片的相对深度，在连续多帧的估计结果中存在尺度不一致问题</li>
</ul>
<h2 id="2-Main-Idea"><a href="#2-Main-Idea" class="headerlink" title="2. Main Idea"></a>2. Main Idea</h2><ul>
<li><p>沿用原有的框架和光度一致性损失进行深度和姿态的无监督训练</p>
</li>
<li><p>利用相邻图像的稀疏特征匹配和对极几何约束，增强图像帧间的几何一致性</p>
</li>
<li><p>在相邻图像间增加深度一致性约束，减少深度图像估计结果中的噪声</p>
</li>
</ul>
<script type="math/tex; mode=display">
L_{total}=\alpha L_{pixel}+(1-\alpha)L_{SSIM}+\beta L_{smooth}+\gamma_1L{epi}+\gamma_2L_{reproj}+\mu_1L_{depth}+\mu_2L_{multi}</script><h2 id="3-Approach"><a href="#3-Approach" class="headerlink" title="3. Approach"></a>3. Approach</h2><p>本论文利用了自监督单目深度估计的基本框架，并在此基础上进行了改进，在更复杂的场景下优化单目深度估计结果。框架由三部分构成，分别是深度和位姿估计网络（光度一致性误差）、对极几何一致性约束模块和深度一致性约束模块。</p>
<p><img src="/2020/09/22/Self-Supervised-Learning-of-Depth-and-Motion-Under-Photometric-Inconsistency/3.png" alt="3"></p>
<h3 id="3-1-Photometric-consistency"><a href="#3-1-Photometric-consistency" class="headerlink" title="3.1 Photometric consistency"></a>3.1 Photometric consistency</h3><p>给定$N$帧连续的单目图像，无监督的深度和自运动估计问题在同时估计目标（中间帧）图像的深度图$D_t$和到$N-1$个源图像的相对姿势$T_{t \to s}=[R_{T \to S}|t_{t \to s}] \in SE(3)$。使用depth CNN和poseCNN进行训练。帧间重建的对应公式为：</p>
<script type="math/tex; mode=display">
p_s \sim K_s[R_{t\to s}|t_{t\to s}]D_t(p_t)K^{-1}_tp_t</script><p>利用帧间相机运动实现源图像（source）到目标图像（target）的重建，计算损失：</p>
<script type="math/tex; mode=display">
L_{pixel}=\frac {1}{|M|}\sum_{\forall p_t \in M}|\hat I^{(s)}_t(p_t|D_t,T_{t \to s})-I_t(p_t)|</script><p>在像素损失（pixel loss）的基础上增加结构相似性损失（SSIM loss）和边缘平滑损失（edge loss），光度一致性损失的计算公式如下：</p>
<script type="math/tex; mode=display">
L_{baseline}=\alpha L_{pixel}+(1-\alpha)L_{SSIM}+\beta L_{smooth}</script><h3 id="3-2-Epipolar-geometric-consistency"><a href="#3-2-Epipolar-geometric-consistency" class="headerlink" title="3.2 Epipolar geometric consistency"></a>3.2 Epipolar geometric consistency</h3><p>上述重建公式需要满足几个假设：</p>
<ul>
<li>建模场景是静态的，没有移动的对象</li>
<li>场景中的表面是朗伯型的</li>
<li>相邻视图之间不存在遮挡</li>
</ul>
<p>由于现实中的绝大多数场景会违背这些假设，所以使用光度一致性损失进行深度估计的结果存在一定的误差。为此，本文提出了一种通过将间接法几何信息注入直接学习的框架来解决此问题的新颖方法。与依赖于密集光度一致性的直接方法不同，视觉SLAM的间接方法基于稀疏的局部描述符，例如SIFT 和ORB 等。局部不变性受比例和光度变化的影响较小，且可以隐式地嵌入到学习框架中。</p>
<p><img src="/2020/09/22/Self-Supervised-Learning-of-Depth-and-Motion-Under-Photometric-Inconsistency/5.png" alt="5" style="zoom: 50%;"></p>
<h4 id="3-2-1-Symmetric-epipolar-error"><a href="#3-2-1-Symmetric-epipolar-error" class="headerlink" title="3.2.1 Symmetric epipolar error"></a>3.2.1 Symmetric epipolar error</h4><p>对极几何约束：</p>
<p><img src="/2020/09/22/Self-Supervised-Learning-of-Depth-and-Motion-Under-Photometric-Inconsistency/4.png" alt="4" style="zoom: 25%;"></p>
<script type="math/tex; mode=display">
p_2^tK^{-T}t\times RK^{-1}p_1=0</script><p>令本质矩阵$E=t \times R$，$x_1=K^{-1}p_1$，$x_2=K^{-1}p_2$，则对极约束可以化简为：</p>
<script type="math/tex; mode=display">
x_2^TEx_1=0</script><p>在相机针孔模型中，目标图像和源图像之间的特征匹配$S_{t \harr s}={\{}p \harr p^{‘}{\}}$满足对极约束，其中$p$和$p^{‘}$是校准后的图像坐标。利用对极约束，对于相邻两帧图像中多对匹配的特征点$p$ ，$p^{‘}$，计算<strong>symmetric epipolar error</strong>：</p>
<script type="math/tex; mode=display">
L_{epi}(S|R,t)=\sum_{\forall (p,p^{'}) \in S}(\frac {p^{'T}Ep}{\sqrt{(Ep)^2_{(1)}+(Ep)^2_{(2)}}} + \frac {p^{T}Ep^{'}}{\sqrt {(Ep^{'})^2_{(1)}+(Ep^{'})^2_{(2)}}})</script><h4 id="3-2-2-Re-projection-error"><a href="#3-2-2-Re-projection-error" class="headerlink" title="3.2.2 Re-projection error"></a>3.2.2 Re-projection error</h4><p><img src="/2020/09/22/Self-Supervised-Learning-of-Depth-and-Motion-Under-Photometric-Inconsistency/6.png" alt="6" style="zoom: 25%;"></p>
<p>为了用特征匹配进行深度估计结果的优化，可使用估计的深度在一个图像中反投影2D特征以计算3D轨迹，然后将3D轨迹重新投影到另一幅图像以计算重新投影误差。由于特征点$p$的坐标不是整数，因此使用$\hat D_t(p)$在目标深度图像上进行双线性采样。</p>
<script type="math/tex; mode=display">
L_{reproj}(S|R,t,D_t)=\sum _{\forall p \harr p^{'} \in S}||[R|t]\hat D_t(p)p-p^{'}||_2</script><p>设计卷积神经网络的几何误差，将所有匹配项的对极误差和重投影误差降至最低，可模仿传统SLAM中的非线性姿态估计策略。</p>
<h3 id="3-3-Consistent-Depth-Estimation"><a href="#3-3-Consistent-Depth-Estimation" class="headerlink" title="3.3 Consistent Depth Estimation"></a>3.3 Consistent Depth Estimation</h3><p>在深度估计模块中，损失函数从源帧到目标帧成对进行计算，即使位姿估计网络一次输出$N-1$个相对姿势，也无法确定这些相对姿势是否按相同比例对齐。本文提出了运动一致深度估计公式来解决此问题。</p>
<h4 id="3-3-1-Forward-backward-consistency"><a href="#3-3-1-Forward-backward-consistency" class="headerlink" title="3.3.1 Forward-backward consistency"></a>3.3.1 Forward-backward consistency</h4><p>本文提出了单目图像的前后一致性。除了双线性采样像素值外，还估计出当前帧的前向和后向图像的深度图。这个过程产生了两个合成的深度图，它们可以用来约束目标图像深度图$\widetilde D^{(s)}_t$的估计。由于在单目估计中深度仅按比例确定，因此在限制深度之前须对深度按比例进行归一化对齐。前后一致性损失计算公式为：</p>
<script type="math/tex; mode=display">
L_{depth}=\frac{1}{|M|}\sum _{\forall p\in M}|\frac{mean(D_t·M)}{mean(\widetilde D^{(s)}_t·M)}·\widetilde D^{(s)}_t(p)-D_t(p)|</script><h4 id="3-3-2-Multi-view-consistency"><a href="#3-3-2-Multi-view-consistency" class="headerlink" title="3.3.2 Multi-view consistency"></a>3.3.2 Multi-view consistency</h4><p>上述损失函数的设计都是基于单帧进行深度估计，为了增强长视频序列的多帧尺度一致性，本文提出了多视角一致性损失，该损失以目标图像（中间帧图像）为尺度对齐的纽带，惩罚项计算了前向深度和后向深度的不一致损失。</p>
<p>形式上，给定连续图像$(I_1,I_2,I_3)$，$I_2$为目标图像，对应的深度估计结果为$(D_1,D_2,D_3)$，位姿估计结果为$(T_{2 \to 1},T_{2 \to 3})$，归一化深度图计算公式为$\overline D_1=s_{12}·D_1$，$I_1$到$I_3$的位姿变化为$T_{1 \to 3}=T_{2 \to 1}^{-1}·T_{2 \to 3}$。多视图损失将深度一致性项和光度一致性项最小化为：</p>
<script type="math/tex; mode=display">
L_{multi}=\alpha L_{pixel}(I_1,\widetilde I^{(3)}_1)+(1-\alpha)L_{SSIM}(I_1,\widetilde I^{(3)}_1)+\frac{1}{|M_{13}|}\sum_{\forall p\in M_{13}}|\overline D_1(p)-\overline D^{(3)}_1(p)|</script><p>$\widetilde I^{(3)}_1$和$\overline D^{(3)}_1$是给定$\overline D_3$ 和$T_{1 \to 3}$之后通过计算得到的重建图像。$L_{multi}$更优于成对损失$L_{pixel}$，$L_{SSIM}$，$L_{epi}$和$L_{depth}$，它利用了链式姿态计算，将两个相对姿势调整到相同的比例，通过对齐多对连续图像来促进增量定位，优化了单目SLAM的结果。</p>
<h2 id="4-Experiment"><a href="#4-Experiment" class="headerlink" title="4. Experiment"></a>4. Experiment</h2><h3 id="4-1-Depth-Estimation"><a href="#4-1-Depth-Estimation" class="headerlink" title="4.1 Depth Estimation"></a>4.1 Depth Estimation</h3><p><img src="/2020/09/22/Self-Supervised-Learning-of-Depth-and-Motion-Under-Photometric-Inconsistency/7.png" alt="7"></p>
<p><img src="/2020/09/22/Self-Supervised-Learning-of-Depth-and-Motion-Under-Photometric-Inconsistency/8.png" alt="8"></p>
<h3 id="4-2-Pose-Estimation"><a href="#4-2-Pose-Estimation" class="headerlink" title="4.2 Pose Estimation"></a>4.2 Pose Estimation</h3><p><img src="/2020/09/22/Self-Supervised-Learning-of-Depth-and-Motion-Under-Photometric-Inconsistency/9.png" alt="9" style="zoom:33%;"></p>
<p><img src="/2020/09/22/Self-Supervised-Learning-of-Depth-and-Motion-Under-Photometric-Inconsistency/10.png" alt="10" style="zoom:30%;"></p>
<h2 id="5-Reference"><a href="#5-Reference" class="headerlink" title="5. Reference"></a>5. Reference</h2><p><a href="https://arxiv.org/abs/1909.09115v1" target="_blank" rel="noopener">https://arxiv.org/abs/1909.09115v1</a></p>
<p><a href="https://arxiv.org/abs/1704.07813" target="_blank" rel="noopener">https://arxiv.org/abs/1704.07813</a></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/09/20/Inductive-Representation-Learning-on-Large-Graphs/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="COOLA-LAB">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="COOLA-LAB">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/09/20/Inductive-Representation-Learning-on-Large-Graphs/" itemprop="url">Inductive Representation Learning on Large Graphs</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-09-20T17:30:08+08:00">
                2020-09-20
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%AF%8F%E5%91%A8%E4%B8%80%E4%BC%9A/" itemprop="url" rel="index">
                    <span itemprop="name">每周一会</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="1-问题定位"><a href="#1-问题定位" class="headerlink" title="1 问题定位"></a>1 问题定位</h2><p>在大规模graph上学到的节点<strong>低维embedding</strong>，在很多预测任务中非常有用，如内容推荐、节点分类等 。但是现在大多数方法都是transductive（直推式）学习， 不能直接泛化到未知节点。这些方法是在一个固定的graph上直接学习每个节点的embedding，但是大多情况graph是会演化的，当网络结构改变以及新节点的出现，直推式学习需要重新训练，很难落地在需要快速生成unseen节点embedding的机器学习系统上。本文<strong>提出inductive（归纳式）学习框架—GraphSAGE(Graph SAmple and aggreGatE)，通过训练多个聚合邻居节点特征的function，将GCN扩展成归纳学习任务，从而对unseen节点起到泛化作用</strong>。</p>
<h2 id="2-相关工作"><a href="#2-相关工作" class="headerlink" title="2 相关工作"></a>2 相关工作</h2><p>大部分学习节点embedding的方法有以下三种：</p>
<p>一、基于因子分解的embedding生成方法：近年来有许多使用随机游走统计和基于矩阵分解来生成节点低维embedding的方法，如基于spectral clustering（谱聚类）的方法以及PageRank算法。因为这些embedding算法直接为单个节点训练节点embedding，所以它们本质上是transductive的（即不能泛化到unseen节点），而且需要expensive的额外训练(例如，重新通过随机梯度下降)来对新节点进行预测。</p>
<p>二、图上的监督学习：</p>
<p>除了节点嵌入方法之外，还有大量关于对图结构数据进行监督学习的paper。这包括各种基于核的方法，其中图的特征向量是从不同的graph kernel中派生出来的。本文在概念上受到了这些算法的启发。然而，前面的这些方法试图对整个图(或子图)进行分类，而这篇paper的重点是为单个节点生成有用的embedding。</p>
<p>三、图卷积网络：</p>
<p>近年来，人们提出了几种用于图学习的卷积神经网络结构。这些方法中的大多数不能推广到大规模的图，或者是为图分类而设计的。然而，本文的方法与Kipf等人引入的图卷积网络(GCN)密切相关。原来的GCN算法是为transductive设置下的半监督学习而设计的，该算法要求在训练过程中全图Laplacian矩阵是已知的，本文可以看作是GCN框架对inductive设置的扩展。</p>
<h2 id="3-文章工作"><a href="#3-文章工作" class="headerlink" title="3 文章工作"></a>3 文章工作</h2><p>本文提出GraphSAGE框架的核心是如何聚合节点邻居特征信息，本章先<strong>介绍GraphSAGE前向传播过程</strong>（生成节点embedding），<strong>不同的聚合函数</strong>设定；然后介绍<strong>无监督和有监督的损失函数</strong>以及<strong>参数学习。</strong></p>
<h3 id="3-1-前向传播"><a href="#3-1-前向传播" class="headerlink" title="3.1 前向传播"></a>3.1 前向传播</h3><p><strong>a. 可视化例子：</strong>下图是GraphSAGE 生成目标节点（红色）embededing并供下游任务预测的过程：</p>
<p><img src="/2020/09/20/Inductive-Representation-Learning-on-Large-Graphs/v2-7e94e024910274eee88ab3947fd3dff4_1440w.jpg" alt="image-20200924141919374"></p>
<ol>
<li>先对邻居随机采样，降低计算复杂度（图中一跳邻居采样数=3，二跳邻居采样数=5）</li>
<li>生成目标节点emebedding：先聚合2跳邻居特征，生成一跳邻居embedding，再聚合一跳邻居embedding，生成目标节点embedding，从而获得二跳邻居信息。</li>
<li>将embedding作为全连接层的输入，预测目标节点的标签。</li>
</ol>
<p><strong>b. 伪代码:</strong></p>
<p><img src="/2020/09/20/Inductive-Representation-Learning-on-Large-Graphs/image-20200924141947029.png" alt="image-20200924141947029"></p>
<p>4-5行是核心代码，介绍卷积层操作：聚合与节点v相连的邻居（采样）k-1层的embedding，得到第k层邻居聚合特征  $h^k_{N(v)}$，与节点v第k-1层embedding $h^{k-1}_v$拼接，并通过全连接层转换，得到节点v在第k层的embedding  $h^k_{v}$。</p>
<h3 id="3-2-聚合函数"><a href="#3-2-聚合函数" class="headerlink" title="3.2 聚合函数"></a><strong>3.2 聚合函数</strong></h3><p>伪代码第5行可以使用不同聚合函数，本小节介绍五种满足排序不变量的聚合函数：平均、GCN归纳式、LSTM、pooling聚合器。（因为邻居没有顺序，聚合函数需要满足排序不变量的特性，即输入顺序不会影响函数结果）</p>
<p><strong>a.平均聚合：</strong>先对邻居embedding中每个维度取平均，然后与目标节点embedding拼接后进行非线性转换。</p>
<script type="math/tex; mode=display">h^k_{N(v)} = mean({h^{k-1}_u,u\in N(v)})</script><script type="math/tex; mode=display">h^k_v = \sigma(W^k . CONCAT(h^{k-1}_v,h^k_{N(u)}))</script><p><strong>b. 归纳式聚合：</strong>直接对目标节点和所有邻居emebdding中每个维度取平均（替换伪代码中第5、6行），后再非线性转换：</p>
<script type="math/tex; mode=display">h^k_v = \sigma(W^k . mean(\{h^{k-1}_v\} \cup \{h^{k-1}_u,\forall u \in N(v)\}))</script><p><strong>c. LSTM聚合：</strong>LSTM函数不符合“排序不变量”的性质，需要先对邻居随机排序，然后将随机的邻居序列embedding  $\{x_t,t\in N(v)\}$作为LSTM输入。</p>
<p><strong>d. Pooling聚合器:</strong>先对每个邻居节点上一层embedding进行非线性转换（等价单个全连接层，每一维度代表在某方面的表示（如信用情况）），再按维度应用 max/mean pooling，捕获邻居集上在某方面的突出的／综合的表现 以此表示目标节点embedding。</p>
<script type="math/tex; mode=display">h^k_{N(v)} = max(\{\sigma(W_{pool}{h^k_{u_i}+b,\forall u_i\in N(v)})</script><script type="math/tex; mode=display">h^k_v = \sigma(W^k . CONCAT(h^{k-1}_v,h^k_{N(u)}))</script><h3 id="3-3-无监督和有监督损失设定"><a href="#3-3-无监督和有监督损失设定" class="headerlink" title="3.3 无监督和有监督损失设定"></a><strong>3.3 无监督和有监督损失设定</strong></h3><p>损失函数根据具体应用情况，可以使用<strong>基于图的无监督损失</strong>和<strong>有监督损失</strong>。</p>
<p><strong>a. 基于图的无监督损失：</strong>希望节点u与“邻居”v的embedding也相似（对应公式第一项），而与“没有交集”的节点$v_n$不相似（对应公式第二项)。</p>
<script type="math/tex; mode=display">J_G(z_u) = -log(\sigma(z^T_uz_v)) - Q.E_{v_n \sim P_n(v)}log(\sigma(-z^T_uz_{v_n}))</script><ul>
<li>$z_u$为节点u通过GraphSAGE生成的embedding。</li>
<li>节点v是节点u随机游走访达“邻居”。</li>
<li>$v_n \sim P_n(u)$表示负采样：节点 $v_n$是从节点u的负采样分布 $P_n$ 采样的，Q为采样样本数。</li>
<li>embedding之间相似度通过向量点积计算得到</li>
</ul>
<p><strong>b. 有监督损失：</strong>无监督损失函数的设定来学习节点embedding 可以供下游多个任务使用，若仅使用在特定某个任务上，则可以替代上述损失函数符合特定任务目标，如交叉熵。</p>
<h3 id="3-4-参数学习"><a href="#3-4-参数学习" class="headerlink" title="3.4 参数学习"></a>3.4 参数学习</h3><p>通过前向传播得到节点u的embedding  $z_u$ ,然后梯度下降（实现使用Adam优化器） <strong>进行反向</strong>传播优化参数 $W^k$ 和聚合函数内参数。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/09/13/Dynamic-Risk-Density-for-Autonomous-Navigation-in-Cluttered-Environments-without-Object-Detection/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="COOLA-LAB">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="COOLA-LAB">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/09/13/Dynamic-Risk-Density-for-Autonomous-Navigation-in-Cluttered-Environments-without-Object-Detection/" itemprop="url">Dynamic Risk Density for Autonomous Navigation in Cluttered Environments without Object Detection</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-09-13T19:00:00+08:00">
                2020-09-13
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%AF%8F%E5%91%A8%E4%B8%80%E4%BC%9A/" itemprop="url" rel="index">
                    <span itemprop="name">每周一会</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>这篇论文提出了一种叫做“动态风险密度”（Dynamic Risk Density）的方法，用于解决在没有目标检测的杂乱环境下进行导航的问题。</p>
<h2 id="1-Previous-work"><a href="#1-Previous-work" class="headerlink" title="1. Previous work"></a>1. Previous work</h2><p><img src="/2020/09/13/Dynamic-Risk-Density-for-Autonomous-Navigation-in-Cluttered-Environments-without-Object-Detection/1.jpg" style="zoom:33%;"></p>
<p>在 Navigating Congested Environments with Risk Level Sets 这篇论文中，作者提出了占据风险代价函数：</p>
<script type="math/tex; mode=display">H(q,x,\dot{x})=\sum^{n}_{i=1}{\frac{\exp(-(q-x_i)^T\Omega(q-x_i))}{1+\exp(-\alpha\dot{x_i}^T(q-x_i))}},\Omega=diag\{\frac{1}{\sigma^2_x},\frac{1}{\sigma^2_y}\} \tag{1}</script><p>其中，</p>
<ul>
<li>$q$：表示二维平面内的任意一个点；</li>
<li>$x$：表示二维平面内所有障碍物的位置，例如图(a)中的四个点；</li>
<li>$\dot{x}$：表示二维平面内所有障碍物的速度，例如图(a)中四个点上的箭头。</li>
</ul>
<p>通过计算 $H$，我们可以得到平面内每个点的占据风险代价，可以将其看成是该点在下一个时刻会出现障碍物的概率。在导航的过程中，为了不发生碰撞，自然要避免经过 $H$ 值大的点。</p>
<p>$H$ 是由一个高斯峰 $\exp(-(q-x_i)^T\Omega(q-x_i))$ 乘上一个 logistic 函数 $\frac{1}{1+\exp(-\alpha\dot{x_i}^T(q-x_i))}$ 构成。很显然，离障碍物越近的点 $H$ 值会越大，这由高斯峰来进行描述，logistic 函数则是对这个高斯峰在速度方向上进行了偏移，如图(b)所示。</p>
<p>在导航的过程中，给定一个阈值 $H_p$，只要确保只在 $H\leq H_p$ 的区域内行动就能避免发生碰撞。</p>
<h2 id="2-Main-Idea"><a href="#2-Main-Idea" class="headerlink" title="2. Main Idea"></a>2. Main Idea</h2><p>在 Previous work 提到的方法中，我们假设已知了环境中所有障碍物的位置和速度，但在现实场景下，这往往是很难得到的。因此，在 Dynamic Risk Density for Autonomous Navigation in Cluttered Environments without Object Detection 这篇论文中，作者对其进行了改进，分别用“占据密度”(occupancy density)和“速度场”(velocity flow field)代替了障碍物的位置和速度。提出了动态风险密度：</p>
<script type="math/tex; mode=display">H_\rho(q,t,\rho,V)=\frac{\rho(q,t)}{1+\exp(\alpha\nabla\rho(q,t)\cdot V(q,t))} \tag{2}</script><p>其中，</p>
<ul>
<li>$\rho$：表示占据密度；</li>
<li>$V$：表示速度场。</li>
</ul>
<p>$H_\rho$ 和 $H$ 的意义类似。</p>
<p><img src="/2020/09/13/Dynamic-Risk-Density-for-Autonomous-Navigation-in-Cluttered-Environments-without-Object-Detection/2.jpg" style="zoom:33%;"></p>
<p>通过仿真可以看出，由这两种方法得到的轮廓线非常相似（左图是计算 $H$ 值得到的，右图是计算 $H_\rho$ 值得到的）。</p>
<h2 id="3-Approach"><a href="#3-Approach" class="headerlink" title="3. Approach"></a>3. Approach</h2><p>在这篇论文中，通过使用一个二维激光扫描仪来构建环境的占据网格而得到占据密度。</p>
<p>对于如何计算速度场，论文中提出了两种方法，分别是基于<strong>聚类</strong>的方法和基于<strong>密度流</strong>的方法。</p>
<h3 id="3-1-Velocity-Field-Estimation-from-Density-Flow"><a href="#3-1-Velocity-Field-Estimation-from-Density-Flow" class="headerlink" title="3.1 Velocity Field Estimation from Density Flow"></a>3.1 Velocity Field Estimation from Density Flow</h3><p>基于聚类的方法，使用 k-means 算法对占据网格进行聚类，并计算聚类中心的速度，再通过 Voronoi 分布将聚类中心的速度映射到所有的点上。</p>
<p><img src="/2020/09/13/Dynamic-Risk-Density-for-Autonomous-Navigation-in-Cluttered-Environments-without-Object-Detection/3.jpg" style="zoom:33%;"></p>
<h3 id="3-2-Velocity-Field-Estimation-from-Density-Flow"><a href="#3-2-Velocity-Field-Estimation-from-Density-Flow" class="headerlink" title="3.2 Velocity Field Estimation from Density Flow"></a>3.2 Velocity Field Estimation from Density Flow</h3><p>基于密度流的方法与计算机视觉中的光流法类似。</p>
<p>在这里引入了两个假设：</p>
<ol>
<li>密度不变性，即同一个点在经过 $\Delta t$ 时间移动了 $\Delta x$， $\Delta y$ 之后，密度没有发生改变：</li>
</ol>
<script type="math/tex; mode=display">\rho^t(x,y)=\rho(x,y,t)=\rho(x+\Delta x,y+\Delta y,t+\Delta t) \tag{3}</script><ol>
<li>对于 $\rho(x+\Delta x,y+\Delta y,t+\Delta t)$ 可以进行一阶泰勒展开：</li>
</ol>
<script type="math/tex; mode=display">\rho(x+\Delta x,y+\Delta y,t+\Delta t)=\rho(x,y,t)+\frac{\partial\rho}{\partial x}\Delta x+\frac{\partial\rho}{\partial y}\Delta y+\frac{\partial\rho}{\partial t}\Delta t+H.O.T. \tag{4}</script><p>结合(3)(4)可以得到：</p>
<script type="math/tex; mode=display">\frac{\partial\rho}{\partial x}\frac{\Delta x}{\Delta t} +\frac{\partial\rho}{\partial y}\frac{\Delta y}{\Delta t}+\frac{\partial\rho}{\partial t}=0 \tag{5}</script><p>对(5)进行变量替换：</p>
<script type="math/tex; mode=display">u=\frac{\Delta x}{\Delta t},v=\frac{\Delta y}{\Delta t}</script><script type="math/tex; mode=display">I_x=\frac{\partial\rho}{\partial x},I_y=\frac{\partial\rho}{\partial y},I_t=\frac{\partial\rho}{\partial t}</script><p>化简可得：</p>
<script type="math/tex; mode=display">\left[\begin{matrix} I_x&I_y \end{matrix}\right] \left[\begin{matrix} u\\v \end{matrix}\right] = -I_t \tag{6}</script><p>这是一个二元一次方程，无法进行求解，因此，必须引入额外的约束来计算 u,v。我们假设某一个窗口内的点具有相同的运动。</p>
<p>考虑一个大小为 $w*w$ 大小的窗口，它含有 $w^2$ 数量的像素。由于该窗口内像素具有 同样的运动，因此我们共有 $w^2$ 个方程： </p>
<script type="math/tex; mode=display">\left[\begin{matrix} I_x&I_y \end{matrix}\right]_k \left[\begin{matrix} u\\v \end{matrix}\right] = -I_{tk}, k=1,2,...,w^2 \tag{7}</script><p>记：</p>
<script type="math/tex; mode=display">A=\left[\begin{matrix}[I_x,I_y]_1\\ \vdots\\ [I_x,I_y]_k\end{matrix}\right],b=\left[\begin{matrix}I_{t1}\\ \vdots\\ I_{tk}\end{matrix}\right]</script><p>于是整个方程为：</p>
<script type="math/tex; mode=display">A\left[\begin{matrix}u\\v\end{matrix}\right]=-b \tag{8}</script><p>利用最小二乘法可以求得：</p>
<script type="math/tex; mode=display">\left[\begin{matrix}u\\v\end{matrix}\right]^*=-(A^TA)^{-1}A^Tb \tag{9}</script><h3 id="3-3-Navigation-Algorithm"><a href="#3-3-Navigation-Algorithm" class="headerlink" title="3.3 Navigation Algorithm"></a>3.3 Navigation Algorithm</h3><p><img src="/2020/09/13/Dynamic-Risk-Density-for-Autonomous-Navigation-in-Cluttered-Environments-without-Object-Detection/4.jpg" style="zoom:33%;"></p>
<p>Experiment：<a href="https://www.youtube.com/watch?v=uXry23LxpWw" target="_blank" rel="noopener">https://www.youtube.com/watch?v=uXry23LxpWw</a></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/09/10/Learning-RoI-Transformer-for-Detecting-Oriented-Objects-in-Aerial-Images/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="COOLA-LAB">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="COOLA-LAB">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/09/10/Learning-RoI-Transformer-for-Detecting-Oriented-Objects-in-Aerial-Images/" itemprop="url">Learning RoI Transformer for Detecting Oriented Objects in Aerial Images</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-09-10T13:22:00+08:00">
                2020-09-10
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%AF%8F%E5%91%A8%E4%B8%80%E4%BC%9A/" itemprop="url" rel="index">
                    <span itemprop="name">每周一会</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="1-问题定义"><a href="#1-问题定义" class="headerlink" title="1.问题定义"></a>1.问题定义</h2><p>目标检测是计算机视觉领域的一个基本问题, 其目标是在场景中快速、精确地定位、识别特定目标，从而为很多计算机视觉应用场景提供重要的信息基础。近年来，该领域随着深度学习技术的快速发展取得了显著的突破，然而由于俯瞰视角、高度复杂的背景以及物体多样的外观，航拍图像中的物体检测是计算机视觉中一项具有挑战性的任务。特别是当在航拍图像中检测密集的物体时，依赖于水平候选框（Horizontal Region of Interest ，HRoI)的普通目标检测的方法经常引入候选框和物体之间的不匹配，并最终导致物体分类和定位的误差。</p>
<h2 id="2-相关工作"><a href="#2-相关工作" class="headerlink" title="2.相关工作"></a>2.相关工作</h2><p>由于基于HRoI的方法总是从水平的特征图中提取特征并进行边框回归，因此对于旋转的物体并不能达到很好的精度。在此之前也有一些方法使用Rotated anchor 来解决这个问题，但是它们的设计总是使 anchor 的数量倍增并且显著增加了计算复杂性。</p>
<h2 id="3-本文工作"><a href="#3-本文工作" class="headerlink" title="3.本文工作"></a>3.本文工作</h2><p>本文针对旋转的物体仍然使用Horizontal anchors，在有有向边界框标注的前提下，提出新的方法 RoI Transformer。核心思想是先生成HRoI,再利用空间特征信息学习到偏转角度参数，最后作相应的调整以达到旋转不变性，同时解决了大量的anchor设计以及特征不匹配的问题。</p>
<p>本文基于 Light Head R-CNN 进行改进，提出了两个新的模块：</p>
<ul>
<li><p>RRoI Learner ：此模块会将 HRoI 传递给 PS RoI Align 层， 再接一个轻量的维度为5的全连接层，回归 Rotated Ground Truths 相对于HRoIs的偏移, 通过解码器将 HRoI 和偏移量作为输入并输出的 RRoI:</p>
<script type="math/tex; mode=display">
  (t_x,t_y,t_w,t_h,t_\theta）</script></li>
<li><p>RRoI Transformer：经过RRoI Learner学习到了旋转参数后，我么在这里就能扭曲特征图以保持深度特征的旋转不变性。RRoI Learner 和 RRoI 变形的组合构成了 RoI Transformer ，然后使用来自 RoI Transformer 的几何鲁棒合并的特征进行 RRoI 的分类和回归。</p>
</li>
</ul>
<p><img src="/2020/09/10/Learning-RoI-Transformer-for-Detecting-Oriented-Objects-in-Aerial-Images/architecture.png" alt></p>
<p>由于我们想要实现物体的旋转不变性特征，而通用检测的损失函数在这里就会出现问题了，在不同角度的情况下，两个旋转幅度相同的RRoI和RGT却会出现不同的横纵坐标偏移量。于是作者基于局部坐标系，对损失函数进行了修改：</p>
<script type="math/tex; mode=display">
t^*_x=1/w_r((x^*-x_r)cos\theta_r+(y^*-y_r)sin\theta_r),</script><script type="math/tex; mode=display">
t^*_y=1/h_r((y^*-y_r)cos\theta_r-(x^*-x_r)sin\theta_r),</script><script type="math/tex; mode=display">
t^*_\theta=1/2\pi((\theta^*-\theta_r) mod2 \pi)</script><p>这样我们通过满足物体相对偏移的旋转不变性来实现了物体特征的旋转不变性，并学习到旋转角度。</p>
<p>本文通过 RoI Transformer 模块模拟几何变换并解决区域特征和物体的不匹配问题，为具有挑战性的 DOTA 和 HRSC 数据集的旋转物体检测带来了显著的改进。对于旋转物体，在有有向边界框标注的情况下， RoI Transformer模块更为合理。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/08/30/AdaFrame-Adaptive-Frame-Selection-for-Fast-Video-Recognition/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="COOLA-LAB">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="COOLA-LAB">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/08/30/AdaFrame-Adaptive-Frame-Selection-for-Fast-Video-Recognition/" itemprop="url">AdaFrame - Adaptive Frame Selection for Fast Video Recognition</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-08-30T16:57:38+08:00">
                2020-08-30
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%AF%8F%E5%91%A8%E4%B8%80%E4%BC%9A/" itemprop="url" rel="index">
                    <span itemprop="name">每周一会</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="1-Introduce"><a href="#1-Introduce" class="headerlink" title="1 Introduce"></a>1 Introduce</h2><p>本文中的场景为视频的快速描述，问题的输入是一段视频序列，输出是视频的类别或标签。目标为efficient video recognition。这类问题一般会对视频均匀采样取帧，这默认了视频的信息是均匀分布的，但实际上并非如此，均匀采样会取到无用的背景帧或冗余的信息帧。</p>
<p><img src="/2020/08/30/AdaFrame-Adaptive-Frame-Selection-for-Fast-Video-Recognition/1.png" alt></p>
<p>分类任务的类别不同，需要的帧数通常也不同，静态物体的识别一般只需要一帧，而复杂的动作识别需要多帧，同类别间也有可能需要不同帧数。 </p>
<p><img src="/2020/08/30/AdaFrame-Adaptive-Frame-Selection-for-Fast-Video-Recognition/2.png" alt></p>
<h2 id="2-Approach"><a href="#2-Approach" class="headerlink" title="2 Approach"></a>2 Approach</h2><p><img src="/2020/08/30/AdaFrame-Adaptive-Frame-Selection-for-Fast-Video-Recognition/3.png" alt></p>
<p>此图为网络的整体结构图。memory-agumented LSTM的输入是一个视频帧序列，在每个step中，将</p>
<ol>
<li>当前帧的特征vt</li>
<li>前一个step的状态h(t-1)、c(t-1)</li>
<li>从golbal memory中得到的Global context ut</li>
</ol>
<p>输入到LSTM中，得到当前step的隐藏状态h(t)，之后h(t)用来计算</p>
<ol>
<li>通过Prediction Network得到prediction向量st，用来计算Reward</li>
<li>通过Selection Network得到action a，用来决定下一步看哪一帧图像</li>
<li>通过Utility Network得到utility vt，用来判断继续看下一帧的预期收益</li>
</ol>
<h3 id="2-1-Memory-agumented-LSTM"><a href="#2-1-Memory-agumented-LSTM" class="headerlink" title="2.1 Memory-agumented LSTM"></a>2.1 Memory-agumented LSTM</h3><script type="math/tex; mode=display">
h_{t},c_{t}=LSTM([v_{t},u_{t}],h_{t-1},c_{t-1})</script><p>帧特征 vt 跟 全局上下文特征 ut 拼接起来作为当前LSTM单元的输入。</p>
<h4 id="Global-memory"><a href="#Global-memory" class="headerlink" title="Global memory"></a>Global memory</h4><p>global memory部分用于提供上下文信息，它的输入是经过空间和时间上降采样取到的帧的集合，用一个lightweight CNN分别提取他们的特征.</p>
<script type="math/tex; mode=display">
M=[v_1^s,v_1^s,...,v_{T_d}^s]</script><p>M为特征集合，由于只是分别对各个帧使用2D CNN，没有利用到时间信息，因此使用PE（position encoding）的方法来将位置信息嵌入到frames represetations中。</p>
<script type="math/tex; mode=display">
z_{t,j}=(W_hh_{t-1})^TPE(v_j^s)</script><p>使用soft-attention来获取global context information。根据上一个时刻的隐藏状态h(t-1)来给每个frame representation加上attention权值</p>
<script type="math/tex; mode=display">
\beta _t=Softmax(z_t)</script><script type="math/tex; mode=display">
u_t = \beta _t^TM</script><h4 id="Reward-function"><a href="#Reward-function" class="headerlink" title="Reward function"></a>Reward function</h4><script type="math/tex; mode=display">
    r_t=max\{0,m_t - max_{t^, \in[0,t-1]} m_{t^,}\}</script><script type="math/tex; mode=display">
    m_t=s_t^{g^t}-max\{s_t^{c^,}|c^,\ne gt \}</script><p>mt代表预测出的对于ground-true类别的可能性比其他类别可能性最大值之间的差距。优化过程中会增大这个差距。Reward函数激励这个margin比历史margin更大，这样设计reward的目的是衡量这一帧是否有助于增大ground-true class的probability，这个帧是否是有意义的。</p>
<h4 id="Selection-network"><a href="#Selection-network" class="headerlink" title="Selection network"></a>Selection network</h4><p>Selecttion network用来决定下一个step看哪一帧。</p>
<script type="math/tex; mode=display">
f_s(h_t;W_s)=a_t = sigmoid(W_s^Th_t)</script><p>直接用一个全连接层将隐藏状态ht映射到一个value上。然后再加上一个高斯分布采样（方差固定为0.01）。加入高斯分布的目的是在训练过程中加入噪声。在inference阶段不用高斯分布采样，直接用at，乘以总帧数就是下一帧的位置。可以发现下一帧不一定是后面的帧，也可以是前面的帧。</p>
<p>这样训练selection network来最大化期望reward：</p>
<script type="math/tex; mode=display">
J_{sel}(W_s)=E_{l_t~\pi(.|h_t;W_s)[\sum _{t=0}^{T_e}r_t]}</script><h4 id="Utility-network"><a href="#Utility-network" class="headerlink" title="Utility network"></a>Utility network</h4><script type="math/tex; mode=display">
f_u(h_t;W_u)=\hat V_t =W_u^Th</script><p>用一个全连接层ht映射为Vt，代表approximation of expected future rewards，其中expected future rewards可以采用公式计算：</p>
<script type="math/tex; mode=display">
V_t=E_{h_t{t+1}:T_e,a_t:T_e}[\sum_{t=0}^{T_e-t}\gamma^ir_{t+i}]</script><p>通过损失函数来训练：</p>
<script type="math/tex; mode=display">
L_{utl}(W_u)= {1\over 2}||\hat V_t-V_t||_2</script><h4 id="Optimization"><a href="#Optimization" class="headerlink" title="Optimization"></a>Optimization</h4><p>综合上述损失函数，最终的目标函数为：</p>
<script type="math/tex; mode=display">
minimize_\Theta L_{cls}+\lambda L_{utl}-\lambda J_{sel}</script><p>前两项可微，可以使用SGD反向传播更新参数即可。</p>
<p>分析第三项的梯度如下：</p>
<script type="math/tex; mode=display">
\nabla _\Theta J_{sel} = E[\sum _{t=0}^{T_e}(R_t-\hat V_t)\nabla _\Theta log\pi _\Theta(.|h_t)]</script><p>vt作为baseline减小方差。Rt代表expected future reward。在一个mini-batch中用Monte-Carlo sampling来拟合。</p>
<h3 id="2-2-Adaptive-Lookahead-Inference"><a href="#2-2-Adaptive-Lookahead-Inference" class="headerlink" title="2.2 Adaptive Lookahead Inference"></a>2.2 Adaptive Lookahead Inference</h3><p>前面讲到的utility network用来预测expected future rewards，看到将来取更多帧所带来的好处。因此根据utility network的输出来决定继续或者停止取帧。在每个时刻都计算vt，并记录vt的最大值，如果有p次当前值比最大值大一个阈值margin，则可停止。</p>
<h2 id="3-Experiment"><a href="#3-Experiment" class="headerlink" title="3 Experiment"></a>3 Experiment</h2><h3 id="Dataset"><a href="#Dataset" class="headerlink" title="Dataset"></a>Dataset</h3><ul>
<li><p>FCVID：91, 223 videos from YouTube. 共239个类别，平均167秒。trainset：45, 611 testset：45, 612</p>
</li>
<li><p>ACTIVITYNET：20K videos，共200个类别。平均117秒。train,val,test=10, 024 vs 4, 926 vs 5, 044。用val作为test。</p>
</li>
</ul>
<h3 id="Main-Result"><a href="#Main-Result" class="headerlink" title="Main Result"></a>Main Result</h3><p><img src="/2020/08/30/AdaFrame-Adaptive-Frame-Selection-for-Fast-Video-Recognition/4.png" alt></p>
<p><img src="/2020/08/30/AdaFrame-Adaptive-Frame-Selection-for-Fast-Video-Recognition/5.png" alt></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/08/11/Tagoram-Real-Time-Tracking-of-Mobile-RFID-Tags-to-High-Precision-Using-COTS-Devices/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="COOLA-LAB">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="COOLA-LAB">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/08/11/Tagoram-Real-Time-Tracking-of-Mobile-RFID-Tags-to-High-Precision-Using-COTS-Devices/" itemprop="url">Tagoram: Real-Time Tracking of Mobile RFID Tags to High Precision Using COTS Devices</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-08-11T21:19:19+08:00">
                2020-08-11
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%AF%8F%E5%91%A8%E4%B8%80%E4%BC%9A/" itemprop="url" rel="index">
                    <span itemprop="name">每周一会</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="RFID-Technology-Background"><a href="#RFID-Technology-Background" class="headerlink" title="RFID Technology Background"></a>RFID Technology Background</h3><p>RFID 是 Radio Frequency Identification 的缩写，即射频识别。RFID 射频识别是一种非接触式的自动识别技术，其基本原理是利用射频信号和空间耦合（电感或电磁耦合）或雷达反射的传输特性，实现对被识别物体的自动识别并获取相关数据，无须人工干预，可工作于各种恶劣环境。RFID 技术可识别高速运动物体并可同时识别多个标签，操作快捷方便。</p>
<p>最基本的 RFID 系统由三部分组成：</p>
<ol>
<li>阅读器(Reader)：读取或写入标签信息的设备，可设计为手持式或固定式，目前国际上主流RFID设备生产商有Impinj，Alien，Zebra。</li>
<li>天线(Antenna)：在标签和读取器间传递射频信号。</li>
<li>标签(Tag)：由耦合元件及芯片组成，每个标签具有唯一的电子编码，附着在物体上标识目标对象；依据电子标签供电方式的不同，电子标签可以分为有源电子标签(Active tag)、无源电子标签(Passive tag)和半无源电子标签(Sem-passive tag)。有源电子标签内装有电池，无源射频标签没有内装电池，半无源电子标签(Sem-passive tag)部分依靠电池工作。专门生产RFID标签的有SMARTRAC。</li>
</ol>
<p>射频识别系统的工作频率基本上划归三个范围：</p>
<ol>
<li>30kHz-300kHz：低频标签一般为无源标签， 其工作能量通过电感耦合方式从阅读器耦合线圈的辐射近场中获得，阅读距离一般情况下小于 1 米。频标签的典型应用有：动物识别、容器识别、工具识别、电子闭锁防盗等。</li>
<li>3MHz ~ 30MHz：中频标签一般也采用无源设主，其工作能量同低频标签一样，阅读距离一般情况下也小于 1 米。中频标签由于可方便地做成卡状，典型应用包括：电子车票、电子身份证、门禁设备等。</li>
<li>超高频（ 300MHz ~ 3GHz）或微波（ &gt;3GHz）：一般可用频段：433.92MHz， 862(902)~928MHz， 2.45GHz， 5.8GHz。标签与阅读器之间的耦合方式为电磁耦合方式。 阅读距离一般大于 1m，典型情况为 4~6m，最大可达 10m 以上。阅读器天线一般均为定向天线，只有在阅读器天线定向波束范围内的射频标签可被读/写。典型应用包括：移动车辆识别、仓储物流应用等。</li>
</ol>
<p>Tracking RFID Application 使用RFID进行跟踪的应用</p>
<ul>
<li>分析顾客消费习惯</li>
<li>运动员路线追踪</li>
<li>人机互动，肢体判断</li>
<li>机器人判断追踪物品位置</li>
<li>机场行李追踪及排序</li>
</ul>
<hr>
<h3 id="Tagoram-Overview"><a href="#Tagoram-Overview" class="headerlink" title="Tagoram Overview"></a>Tagoram Overview</h3><p><strong>思路来源</strong></p>
<p>考虑使用RFID无源标签的反向散射信号中的相位值θ，θ为读写器发送和接收信号的相位差。Impinj  Reader硬件设备检测到的相位值为一个12位的数，相位分辨率可达 $ 2\pi \div 4096 = 0.0015rad $ , Reader的平均波长为320mm，理论上的检测距离可达0.038mm。</p>
<p><img src="https://raw.githubusercontent.com/agnes-yang/i/img/md_img/research_rfid_backscatter communication_20200804.jpg" alt="research_rfid_backscatter communication_20200804"></p>
<h4 id="Challenges"><a href="#Challenges" class="headerlink" title="Challenges"></a>Challenges</h4><ol>
<li>RF相位受热噪声（thermal noise）的影响：在0度到40度的环境下，920 ∼ 926MHz的信道带宽（包含16个通道），RSS从-70 - -30dbm的环境条件下对多个标签测试相位，其满足标准差为0.1的正态分布。如何在不确定的相位信息中得到精确且确定的追踪结果是个问题。</li>
<li>硬件差异会造成额外的相位偏移（diversity term），从而导致标签和读写器的校准无法实现，因为使用不同的硬件需要不同的校准；</li>
<li>环境变化使得相位测量结果复杂。</li>
</ol>
<h4 id="问题定义"><a href="#问题定义" class="headerlink" title="问题定义"></a>问题定义</h4><p>$ A = \{ A_1, A_2, … , A_M \} $ </p>
<p>天线序列，$M$ : 天线数，$A_m$ : 第 m 根天线</p>
<p>$ \Theta=\begin{pmatrix}<br>\theta_{1,1} &amp; … &amp; \theta_{1,N}\\<br>\vdots &amp; \vdots &amp; \vdots\\<br>\theta_{M,1} &amp; \cdots &amp; \theta_{M,N}\\<br>\end{pmatrix} $ </p>
<p>$ \Theta $ : 测得的相位值，$ \theta_{m,n} $ 表示第m根天线第n轮的检测值，$N$ : 总共做 N轮次收集</p>
<p>检测区域分成为 $ W \times L $ 的grids</p>
<script type="math/tex; mode=display">T=\begin{pmatrix}
t_{1,1} & ... & t_{1,N}\\
\vdots & \vdots & \vdots\\
t_{M,1} & \cdots & t_{M,N}\\
\end{pmatrix}=t_0+\begin{pmatrix}
  \triangle_{1,1} & ... & \triangle_{1,N}\\
  \vdots & \vdots & \vdots\\
  \triangle_{M,1} & \cdots & \triangle_{M,N}\\
\end{pmatrix}\,.</script><p>$ T $ : 测得时间戳，$t_0$: 最小的时间，开始记录的时间，$\triangle_{m,n}$: 记录的时间与开始的时间差</p>
<p>找到$\{f(t_{1,1}),f(t_{1,2}),…,f(t_{M,N})\}$ tag 在随时间变化的标签运动轨迹</p>
<hr>
<h3 id="Solution"><a href="#Solution" class="headerlink" title="Solution"></a>Solution</h3><h4 id="Movement-with-known-track"><a href="#Movement-with-known-track" class="headerlink" title="Movement with known track"></a>Movement with known track</h4><p>Movement with known track意为the tags move along a known track with a constant speed，此时只需知道标签的初始位置 $ f(t_0) $ 即可，<strong>virtual antenna matrix</strong> 使用虚拟天线阵列</p>
<p>假设标签不动，天线运动，则天线的运动为天线的初始位置与标签运动的反向叠加。</p>
<p><img src="https://raw.githubusercontent.com/agnes-yang/i/img/md_img/research_ rfid_tagoram_virtual antenna matrix_20200804.jpg" alt="research_ rfid_tagoram_virtual antenna matrix_20200804"></p>
<p>则用 $ A $ 表示虚拟天线阵列， </p>
<script type="math/tex; mode=display">A=\begin{pmatrix}
A_{1,1} & ... & A_{1,N}\\
\vdots & \vdots & \vdots\\
A_{M,1} & \cdots & A_{M,N}\\
\end{pmatrix}</script><p>有 $ A_{m,n}=A_m-\vec V \times\Delta _{m,n} $</p>
<p>提出建立RF hologram( RF全息图 )</p>
<p><strong>全息图含义</strong>：将平面分成毫米级W ×L的网格，并把图心作为每一个网格的坐标，RF全息图显示的是每一个网格成为初始坐标的可能性。用 $ I $ 表示。</p>
<script type="math/tex; mode=display">I=\begin{pmatrix}
x_{1,1} & ... & x_{1,L}\\
\vdots & \vdots & \vdots\\
x_{W,1} & \cdots & x_{W,L}\\
\end{pmatrix}</script><h5 id="Naive-hologram-朴素全息图"><a href="#Naive-hologram-朴素全息图" class="headerlink" title="Naive hologram 朴素全息图"></a>Naive hologram 朴素全息图</h5><p>$ h(X, A) =\frac{4\pi}{\lambda}|X\text{ }A|\text{ mod } 2\pi $ 理论相位</p>
<p>理论相位表示如果X位置存在标签，从天线A发出并在X方格位置反射的理论相位值。</p>
<script type="math/tex; mode=display">x_{w,l}=|\sum_{m=1}^M\sum_{n=1}^N S(X_{w,l}, A_{m,n}, \theta_{m,n})|</script><script type="math/tex; mode=display">S(X, A,\theta) = e^{J(h(X,A)-\theta)}</script><p>$ J $ : 虚数单位 </p>
<p>$ e^{Jθ} $ : 表示振幅为1的负指数</p>
<p>若网格X是初始位置，则理论相位 $ h(X, A) $和测量所得相位 $ \theta $ 相等。因此 $ h(X,A)-\theta $ 接近0，向量 $ e^{J(h(X,A)-\theta)} $ 在正方上接近实轴。所以，当 $ X $ 为标签所处位置时，$ S $ 值在网格 $ X $ 处叠加增强，产生较高的叠加 $ S $ 值。反之， $ h(X,A)-\theta $ 均匀分布在0-360度的范围内，$ S $ 值相加后相互抵消，该网格的 $ S $ 值较小。</p>
<p><img src="https://raw.githubusercontent.com/agnes-yang/i/img/md_img/research_rfid_tagoram_superimposing the observations_20200804.jpg" alt="research_rfid_tagoram_superimposing the observations_20200804"></p>
<ul>
<li><p>测试：A tag is interrogated 220 times by 2 antennas，结果如下图，标签实际位置 (108,68)</p>
<p>  <img src="https://raw.githubusercontent.com/agnes-yang/i/img/md_img/research_rfid_tagoram_naive hologram_20200804.jpg" alt="research_rfid_tagoram_naive hologram_20200804"></p>
</li>
</ul>
<h5 id="Augmented-Hologram-增广全息图"><a href="#Augmented-Hologram-增广全息图" class="headerlink" title="Augmented Hologram 增广全息图"></a>Augmented Hologram 增广全息图</h5><p>定义 $ \text{PSNR}=\frac{x_{w,l}}{\sum_{i=1}^W\sum_{j=1}^L x_{i,j}} $ ，实质就是归一化</p>
<script type="math/tex; mode=display">x_{w,l}=|\sum_{m=1}^M\sum_{n=1}^N ||S(X_{w,l}, A_{m,n}, \theta_{m,n})||S(X_{w,l}, A_{m,n}, \theta_{m,n})|</script><script type="math/tex; mode=display">\begin{cases}
||S(X, A, \theta)||=2\times F(|h(X, A)-\theta|;0,0.1) \\[2ex]
F(x ;\mu, \sigma)=\frac{1}{\sigma \sqrt{2\pi}}\int^{\infty}_{x} \text{exp } (-\frac{(t-\mu)^2}{2\sigma^2}) dt
\end{cases}</script><p>$ ||S|| $ : virtual amplitude 虚拟振幅。使用虚拟振幅将原来概率的各个分量乘以对应与相位差相关的因子，增大相位差小的分量，缩小相位差大的分量，使得图像中的初始位置更突出。</p>
<ul>
<li><p>测试：</p>
<p>  <img src="https://raw.githubusercontent.com/agnes-yang/i/img/md_img/research_rfid_tagoram_augmented hologram_20200804.jpg" alt="research_rfid_tagoram_augmented hologram_20200804"></p>
</li>
</ul>
<h5 id="Differential-Augmented-Hologram-差分增广全息图"><a href="#Differential-Augmented-Hologram-差分增广全息图" class="headerlink" title="Differential Augmented Hologram 差分增广全息图"></a>Differential Augmented Hologram 差分增广全息图</h5><script type="math/tex; mode=display">S(X, A,\theta) = e^{J(h(X,A)-(h(T,A)+c))}</script><script type="math/tex; mode=display">x_{m,l}=|\sum_{m=1}^M\sum_{n=1}^N ||\Bbb S(X_{w,l}, A_{m,n}, \theta_{m,n})||\Bbb S(X_{w,l}, A_{m,n}, \theta_{m,n})|</script><script type="math/tex; mode=display">\begin{cases}
\Bbb S(X_{w,l}, A_{m,n}, \theta_{m,n})=e^{J\theta_{dif}} \\[2ex] ||\Bbb S(X, A, \theta)||=2\times F(|\theta_{dif}|;0,0.1\times \sqrt 2) \\[2ex]
\theta_{dif}=(h(X_{w,l}, A_{m,n})-\theta_{m,n})-h(X_{w,l}, A_{m,1})-\theta_{m,1}))
\end{cases}</script><p>$(h(X_{w,l}, A_{m,n})-\theta_{m,n})-(h(X_{w,l}, A_{m,1})-\theta_{m,1})= \\<br>(h(X_{w,l}, A_{m,n})-(h(T, A_{m,n})+c)- \\   (h(X_{w,l}, A_{m,1})-(h(T, A_{m,1})+c))= \\<br>h(X_{w,l}, A_{m,n})-h(T, A_{m,n})+h(T, A_{m,1})-h(X_{w,l}, A_{m,1}) $</p>
<p>假设标签实际位置为 $ T $ ，当 $ T=X $ ，振幅应该最大，但是因为设备多样性导致的而偏移，加上一个 $ c $ 。每一个设备测得的相位差都被减去其中的第一个元素，通过差分消除各个设备包括相位偏移在内的系统误差。</p>
<ul>
<li><p>测试：</p>
<p>  <img src="https://raw.githubusercontent.com/agnes-yang/i/img/md_img/research_rfid_tagoram_differential augmented hologram_20200804.jpg" alt="research_rfid_tagoram_differential augmented hologram_20200804"></p>
</li>
</ul>
<h5 id="Real-time-track"><a href="#Real-time-track" class="headerlink" title="Real-time track"></a>Real-time track</h5><p>回顾之前的2D全息图，发现在全息图中，大多数像素的PSNR值较低（蓝色），而不需要进行相关计算。如果忽略这些像素，可以减少计算时间。当给定一个被测相位 $ \theta_{m,n} $ ，可以找到一组圆心在 $ A_{m,n} $ 的弧，称为候选网格，可以忽略其他网格来节省计算。使用哈希表实现了该想法。</p>
<p>假设天线初在原点 $ C_{m,n}=\{X|h(X,0)=\theta _{m,n}\} $</p>
<p>考虑噪声影响 $ C_{m,n}=\{X||h(X,0)-\theta_{m,n}| \le \sigma\} $ </p>
<p>加上天线的实际位置 $ C_{m,n}=\{X+\vec{A_{m,n}}||h(X,0)-\theta_{m,n}| \le \sigma\} $</p>
<h4 id="Movement-with-unknown-track"><a href="#Movement-with-unknown-track" class="headerlink" title="Movement with unknown track"></a>Movement with unknown track</h4><h5 id="Fitting-tag’s-trajectory-拟合轨迹"><a href="#Fitting-tag’s-trajectory-拟合轨迹" class="headerlink" title="Fitting tag’s trajectory 拟合轨迹"></a>Fitting tag’s trajectory 拟合轨迹</h5><p>传送带速度可达274.8mm/s，则标签运动速度可达9.16mm，远远小于设备频率对应波长。</p>
<p><strong>通过相邻两次读取的相位差估计标签的径向速度</strong></p>
<p><img src="https://raw.githubusercontent.com/agnes-yang/i/img/md_img/research_rfid_tagoram_modeling tag movement_20200804.jpg" alt="research_rfid_tagoram_modeling tag movement_20200804"></p>
<script type="math/tex; mode=display">\Delta d=\begin{cases}
\frac{\theta_{m,n+1}-\theta_{m,n}}{4\pi}\times \lambda, |\theta_{m,n+1}-\theta_{m,n}| < \pi \\[2ex] 
 \frac{(2\pi -\theta_{m,n+1}-\theta_{m,n})}{4\pi}\times \lambda, \theta_{m,n} - \theta_{m,n+1} \ge \pi \\[2ex]
 \frac{(-2\pi +\theta_{m,n+1}-\theta_{m,n})}{4\pi}\times \lambda, \theta_{m,n} - \theta_{m,n+1} \le \pi
\end{cases}</script><p>估计速度的大小和方向</p>
<script type="math/tex; mode=display">\tilde V_{m,n} \approx \frac{\Delta d}{t_{m, n+1} - t_{m,n}}</script><script type="math/tex; mode=display">\angle \tilde V_{m,n} \approx \angle(f(t_n)-A_m)</script><p>speed chain: </p>
<script type="math/tex; mode=display">|\vec V_{m,n}|=|\vec V_n|\text{ cos}(\angle\vec V_n-\angle\vec V_{m,n})</script><p>每轮找到 min $|V_{m,n}-\tilde V_{m,n}|$ </p>
<p>轨迹函数：</p>
<script type="math/tex; mode=display">f(t_n)=f(t_n-1)+f(t_n-t_{n-1})\cdot \vec V_n=f(t_0)+\sum_{k=1}^n(t_k-t_{k-1})\cdot \vec V_n</script><hr>
<h3 id="Implication-amp-Experiments"><a href="#Implication-amp-Experiments" class="headerlink" title="Implication &amp; Experiments"></a>Implication &amp; Experiments</h3><p>两种情况: controllable case  &amp; uncontrollable case，即可知运动轨道与速度和位置轨道</p>
<h4 id="Evaluation-in-controllable-case"><a href="#Evaluation-in-controllable-case" class="headerlink" title="Evaluation in controllable case"></a>Evaluation in controllable case</h4><ul>
<li>阅读器型号：Impinj Speedway R420，标签：Alien</li>
<li><p>speed of tag ：0.176m/s</p>
</li>
<li><p>Linear and circular track 直线和圆形的轨道</p>
</li>
</ul>
<p><img src="https://raw.githubusercontent.com/agnes-yang/i/img/md_img/research_rfid_tagoram_experiment setups_20200804.jpg" alt="research_rfid_tagoram_experiment setups_20200804"></p>
<h4 id="Accuracy-among-different-methods"><a href="#Accuracy-among-different-methods" class="headerlink" title="Accuracy among different methods"></a>Accuracy among different methods</h4><ul>
<li>Linear track 下测试，对比本文方法与之前的几种方法的精度</li>
</ul>
<p><img src="https://raw.githubusercontent.com/agnes-yang/i/img/md_img/research_rfid_tagoram_accuracy comparison_20200804.jpg" alt="research_rfid_tagoram_accuracy comparison_20200804"></p>
<ul>
<li>RSS: 使用用RFID标签反向散射的RSS值（接收信号强度值），易受到多径效应，天线增益和标签与天线间角度的影响。</li>
<li>Otrack: 使用RSS和读取率read rate判断行李顺序</li>
<li>PinIt: 合成孔径雷达技术，SAR(reader motion)，主要参考标签</li>
<li>BackPos: 双曲定位技术（hyperbolic positioning）</li>
</ul>
<hr>
<h4 id="Tracking-in-circular-track"><a href="#Tracking-in-circular-track" class="headerlink" title="Tracking in circular track"></a>Tracking in circular track</h4><p>下图为圆形轨道测试的CDF（Cumulative Distribution Function）图</p>
<p><img src="https://raw.githubusercontent.com/agnes-yang/i/img/md_img/research_rfid_tagoram_tracking in circular track_20200804.jpg" alt="research_rfid_tagoram_tracking in circular track_20200804"></p>
<h4 id="Accuracy-among-different-holograms"><a href="#Accuracy-among-different-holograms" class="headerlink" title="Accuracy among different holograms"></a>Accuracy among different holograms</h4><ul>
<li><p>Linear track下测试</p>
</li>
<li><p>对NH, DH, DAH三种全息图建模进行了测试</p>
</li>
<li><p>NH的平均精度为600mm，标准偏差为100mm。AH克服了热噪声引起的偏差，降低了60%的偏差。DAH进一步消除了设备多样性的影响。它的第90百分位是18毫米，第99百分位是25毫米。</p>
</li>
</ul>
<p><img src="https://raw.githubusercontent.com/agnes-yang/i/img/md_img/research_rfid_tagoram_different holograms_20200804.jpg" alt="research_rfid_tagoram_different holograms_20200804"></p>
<h4 id="Real-time-performance"><a href="#Real-time-performance" class="headerlink" title="Real-time performance"></a>Real-time performance</h4><ul>
<li>计算量优化</li>
</ul>
<p>下图是生成全息图所需时间的CDF图，绿色的是阅读时间的概率分布曲线，在没有优化的情况下，$ 10^3 \times 10^3 $ 分辨率的全息图所消耗的时间中值为118ms，在每秒读取30次，读取间隔时间约为33ms的条件下间隔，计算时间远远超过这个读取间隔。使用哈希表进行优化的生成与读取时间相比总是保持较低的时间开销，即使分辨率达到 $ 10^5 \times 10^5 $ ，也能达到25ms的中值。</p>
<p><img src="https://raw.githubusercontent.com/agnes-yang/i/img/md_img/research_rfid_tagoram_read time_20200804.jpg" alt="research_rfid_tagoram_read time_20200804"></p>
<ul>
<li>实时性</li>
</ul>
<p>Tagoram需要在收集足够的读数后输出初始位置，那么需要多少次读取才能获得准确的结果？下描绘了准确性和实时性之间的关系。结果表明，当采集到120次以上的读数时，精度趋于稳定。因此，在接收到120个读取之后输出位置结果是合理的。读取120次总共需要120×25ms=2500ms。2.5秒的延迟对于对于机械系统的控制，如传送带或机械手臂的实时应用来说是可以接受的。</p>
<p><img src="https://raw.githubusercontent.com/agnes-yang/i/img/md_img/research_rfid_tagoram_stability_20200804.jpg" alt="research_rfid_tagoram_stability_20200804"></p>
<h4 id="Impacts-of-Parameters"><a href="#Impacts-of-Parameters" class="headerlink" title="Impacts of Parameters"></a>Impacts of Parameters</h4><p>验证其他的一些参数或设置不会影响实验结果，结果如下图，包括：</p>
<ul>
<li><p>频率 Frequency：对920-926MHZ的16个信道进行了测试。实际上，跳频功能（频率的自动变化）有助于改善阅读器和标签的连接</p>
</li>
<li><p>角度 Orientation：角度指标签与天线极化方向的夹角。</p>
</li>
<li><p>距离 Distance：标签到天线的距离。</p>
</li>
</ul>
<p><img src="https://raw.githubusercontent.com/agnes-yang/i/img/md_img/research_rfid_tagoram_impact of frequency_20200804.jpg" alt="research_rfid_tagoram_impact of frequency_20200804"></p>
<p><img src="https://raw.githubusercontent.com/agnes-yang/i/img/md_img/research_rfid_tagotam_impact of orientation_2020804.jpg" alt="research_rfid_tagotam_impact of orientation_2020804"></p>
<p><img src="https://raw.githubusercontent.com/agnes-yang/i/img/md_img/research_rfid_tagoram_impact of distance_20200804.jpg" alt="research_rfid_tagoram_impact of distance_20200804"></p>
<h4 id="Evaluation-in-uncontrollable-case"><a href="#Evaluation-in-uncontrollable-case" class="headerlink" title="Evaluation in uncontrollable case"></a>Evaluation in uncontrollable case</h4><ul>
<li>4个天线，标签沿不规则轨道运动</li>
<li>收集了5分钟的数据，标签沿轨道逆时针运行10圈</li>
<li>测试平均速度 210.2mm/s，实际速度 212mm/s</li>
<li>结果表明在平缓的位置附近比在急转弯位置测试的结果要好（Has a better effect in the smooth part than the curve part），因为在拟合轨迹的时候是将每一段的运动当做匀速直线运动。</li>
</ul>
<p><img src="https://raw.githubusercontent.com/agnes-yang/i/img/md_img/research_rfid_tagoram_top view of the track_20200804.jpg" alt="research_rfid_tagoram_top view of the track_20200804"></p>
<p><img src="https://raw.githubusercontent.com/agnes-yang/i/img/md_img/research_rfid_tagoram_estimated speed_20200804.jpg" alt="research_rfid_tagoram_estimated speed_20200804"></p>
<p><img src="https://raw.githubusercontent.com/agnes-yang/i/img/md_img/research_rfid_tagoram_fitted trajectory_20200804.jpg" alt="research_rfid_tagoram_fitted trajectory_20200804"></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/08/03/Weakly-Supervised-Semantic-Segmentation-with-Boundary-Exploration/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="COOLA-LAB">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="COOLA-LAB">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/08/03/Weakly-Supervised-Semantic-Segmentation-with-Boundary-Exploration/" itemprop="url">Weakly Supervised Semantic Segmentation with Boundary Exploration</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-08-03T19:57:08+08:00">
                2020-08-03
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%AF%8F%E5%91%A8%E4%B8%80%E4%BC%9A/" itemprop="url" rel="index">
                    <span itemprop="name">每周一会</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="1-问题定位"><a href="#1-问题定位" class="headerlink" title="1 问题定位"></a>1 问题定位</h2><p>语义分割作为经典的计算机视觉的问题，从15年的FCN到之后的deeplab等，语义分割模型在自动驾驶，场景理解等场景都有较多的应该场景，同时也对神经网络的解释性产生一定的促进作用。对于输入的RGB图像或者灰度图，模型需要能够输出特定类别的区域，对于未设置的类别均作为背景处理。但是经典的语义分割模型都是使用像素级别的标签做监督，而这种标签在标注时非常耗时耗力，因为大家开始尝试使用弱监督(低于像素级别的监督)来完成语义分割任务，常见的弱监督包括：bounding box / scribble / image-level，本文也是基于image-level，图像级别的标签作为监督。该监督最为常见且监督级别最低，难度也较大。</p>
<h2 id="2-相关工作"><a href="#2-相关工作" class="headerlink" title="2 相关工作"></a>2 相关工作</h2><p>大部分文章的解决思路是使用localization技术获取物体的初始位置信息，之后通过策略将初始定位结果进行优化。可以视为两阶段的localization-coarse。<br>localization方面，经典的技术包括saliency detection和CAM:</p>
<ul>
<li>CAM是使用全卷积网络提取特征，之后使用20个1x1的卷积核过滤，在将20张特征图通过GAP转换为20个标量值，使用多标签损失函数训练之后，20张特征图就对应20个类别的激活图。</li>
<li>saliency map是将分类模型的梯度反向传播回原图得到高激活值的区域即对应类别的位置。在反向传播的过程中，如果一个节点正向传播时没有激活，那么该节点不进行反向传播的梯度传递。<br>  coarse方面:</li>
<li>实现思路各式各样，不过主要要解决的问题是localization的定位面积过小，所以很多工作聚焦在鼓励类别区域扩撒，同时扩散的过程中保持边界性(由CRF完成)。</li>
</ul>
<h2 id="3-文章工作"><a href="#3-文章工作" class="headerlink" title="3 文章工作"></a>3 文章工作</h2><p>对于CAM进行了改进，提出了attention-pooling CAM。</p>
<ul>
<li>全卷积网络之后得到20张特征图$F_c$，对于每张$F_c$得各个位置，乘以参数$k$之后过一层softmax，得到attention map$A_c$。最终对应类别的置信度为特征图与attention map对应位置相乘后相加的值$P_c = \sum(F_c\times A_c)$</li>
<li>在梯度更新的反向传播时，attention map部分的梯度不进行传递。</li>
<li>训练收敛后的$F_c$即对应类别定位图。</li>
</ul>
<p>对于优化部分，目标是显式的预测类别的边界，通过规则将attention-pooling CAM的结果进行过滤，得到少量的标签正样本和较多的负样本以及无关的标签(不参与训练)。</p>
<ul>
<li>构造一个网络用于预测物体边界，属于边界二分类网络。训练数据使用从CAM合成得到的标签数据，损失函数方面由于样本的不均衡性所以做了归一化。如下<script type="math/tex; mode=display">L_{B}=-\sum_{i \in \Phi_{b r y}} \frac{W_{i} \log \left(P_{i}\right)}{\left|\Phi_{b r y}\right|}-\frac{1}{2}\left(\sum_{i \in \Phi_{c}} \frac{\log \left(1-P_{i}\right)}{\left|\Phi_{c}\right|}+\sum_{i \in \Phi_{b g}} \frac{\log \left(1-P_{i}\right)}{\left|\Phi_{b g}\right|}\right)</script></li>
<li>通过训练更多的类别边界得到挖掘，之后使用IRNet文章的工作，将边界图转换为亲和度矩阵，计算每个位置与其邻近像素之间的亲和度关系，作为概率转移矩阵，模拟随机游走扩散，对CAM的结果进行优化。</li>
</ul>
<p>经过优化之后，segmentation的结果就比较不错了，之后使用类似retrain的思路，将生成的语义分割结果和原图以全监督的方式训练deeplab模型，以得到最终的结果。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/07/21/Improving-Law-Enforcement-Daily-Deployment-Through-Machine-Learning-Informed-Optimization-under-Uncertainty/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="COOLA-LAB">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="COOLA-LAB">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/07/21/Improving-Law-Enforcement-Daily-Deployment-Through-Machine-Learning-Informed-Optimization-under-Uncertainty/" itemprop="url">Improving Law Enforcement Daily Deployment Through Machine Learning-Informed Optimization under Uncertainty</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-07-21T21:22:22+08:00">
                2020-07-21
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%AF%8F%E5%91%A8%E4%B8%80%E4%BC%9A/" itemprop="url" rel="index">
                    <span itemprop="name">每周一会</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>今天要讲的这篇论文来自 IJCAI 2019，从标题可以知道，本文研究的问题是执法力量的布置优化问题，涉及到执法资源的分配优化（Law Enforcement Resource Allocation），那么什么是 Law Enforcement Resource Allocation 问题呢？</p>
<p>简单地说，就是将警员安排到划分好的执勤区域中，当区域中发生盗窃，谋杀等事件时，区域中的警员可以快速响应，减少人民群众的生命和财产安全。那么，我们刚刚已经已经说到了执法资源分配中，主要涉及的三个实体：<strong>区域，警察</strong>和<strong>事件</strong>。区域比较简单，就是按照地理或者行政因素划分的若干个互不重叠的小范围。有一些工作会在考虑工作负载等因素的前提下对整个城市进行划分，本文则是在现有的区域划分的基础上进行执法资源的优化。第二个实体是，警察，考虑到目前执法中无人机等智能设备也是一类执法资源，因此我们用 agent 来同一代表执法资源。Agent 的职责就是，当有事件发生时，对事件进行响应，前往事故现场，处理完，然后返回驻地。在本文中，允许 agent 跨区域进行响应，只要这个 agent 是事件发生时是最近可用的执法力量。同时，agent 在区域之间移动所需要的时间是确定，具体的数值可以利用历史数据得到。第三类实体，事件。事件的属性比较多，首先最基础的是发生的时间，地点。此外，不同的事件应当具有不同的优先级，在文中主要划分了紧急和非紧急两类，比如凶杀就是紧急的，有人报告有噪声污染就是非紧急的。不同优先级对应的是不同的服务标准，比如紧急的五分钟需要赶到现场，比如非紧急的十五分钟赶到现场。不同的事件需要的处理人数也不一样，比如聚众斗殴需要大量警力维持秩序，而处理噪声污染举报一般一个人就够了。当一个事件的处理需要 $n$ 个 agent 时，本文限定了一定是最近可用的 $n$ 个 agent 进行响应。这里的最近不是指的地理上的最近，而是时间上的最近，我们考察一个 agent 和一个事件时间上的距离，如果当前 agent 处于空闲状态，那么时间距离就是从驻地赶往事故发生地的时间，而如果 agent 正在处理其他事件，那么时间距离就是 agent 处理当前事件还需要的时间加上从驻地赶往事故发生地的时间。最后一个属性是处理时间，就是等事件需要的执法资源全部到齐后，一起处理事件所需要花费的时间。文中假设，在事件发生时，我们就能够得到准确的处理时间信息。</p>
<p>刚刚我们介绍完了问题相关的三个实体，那么问题优化的目标是什么呢？刚刚介绍过，每个事件都有一个服务指标 QoS，当第一个响应的 agent 能够在事件发生后 QoS 的时间内赶到事故发生地，那么我们认为事件被成功响应，否则，则视为响应失败。本文的优化目标就是要，在给定 agent 数量上限下，通过优化各个区域中 agent 数量的配置，最小化事件的响应失败率。 </p>
<p>要解决以上问题需要面对的挑战主要有两点：</p>
<p>一、这个问题具有较大的随机性，主要体现在事件的数量，时空分布以及其他的属性；</p>
<p>二、问题涉及到的区域数量，agent 数量以及事件数量不是个小数目，问题的规模可能很大；</p>
<p>下面，我们先简要简要介绍一下，本文解决该问题的一个大概思路。优化目标的重点是事件，但是事件是随机的，因此第一步利用机器学习方法，从历史数据中学习事件的分布模式，生成多样化的事件样本集合。第二步，根据优化目标和相关约束，建立一个整数规划模型来描述问题，同时使用样本平均近似的方法来得到一个鲁棒的解。最后，由于整数规划无法适应于大规模问题的求解，因此本文也提出了一个基于迭代区域搜索的启发式策略在相对较短的时间内得到一个可接受的解。</p>
<p>下面，我们对各部分的细节进行讲解。</p>
<h2 id="Incident-Problem-and-Generation"><a href="#Incident-Problem-and-Generation" class="headerlink" title="Incident Problem and Generation"></a>Incident Problem and Generation</h2><p>在事件预测和生成的过程中，使用的机器学习方法是高斯过程，或者更准确的说是高斯过程回归。下面我们简要地介绍一下这个方法。在介绍方法之前，我们首先要回顾一下高斯回归的数学基础，不用担心，今天我主要想讲地是高斯过程背后的原理，至于复杂的数学知识，如果大家有兴趣可以自己去钻研一下。高斯过程，分为高斯和过程，高斯指的是高斯分布，过程指的是随机过程。高斯分布我们大家都应该很熟悉，高斯分布又可以称为正态分布，一位高斯分布，我们也很熟悉，它的概率密度函数，</p>
<script type="math/tex; mode=display">
f(x)=\frac{1}{\sigma \sqrt{2 \pi}} e^{-\frac{1}{2}\left(\frac{x-\mu}{\sigma}\right)^{2}}</script><p>其中，$\mu$ 是分布的均值，$\sigma$ 是分布的标准差。它的函数图像是一个倒扣的钟形，$\mu$ 决定了中心的位置，而 $\sigma$ 决定了形状，$\sigma$ 越大，图像越扁平，越小，图像则越瘦高。</p>
<p>现在，我们看到多元高斯分布，又称作联合正态分布，一组变量 $X$ 服从多元高斯分布，我们可以把它记作：</p>
<script type="math/tex; mode=display">
\mathbf{X} \sim \mathcal{N}(\mu, \Sigma)</script><p>其中 $\mu$ 是一个向量，$\mu_i$ 表示了第 $i$ 个维度的均值，$\Sigma$ 是协方差矩阵，其元素 $\delta_{ij}$ 表示第 $i$ 维和第 $j$ 维变量之间的相关性。所谓的相关性，从直观上理解就是，你增我也增，协方差为正，你增我减，协方差为负，你增减和我不相干，协方差为就为 0。同时，在多元高斯分布中，任意数量维度的组合同样服从多元高斯分布。</p>
<script type="math/tex; mode=display">
\operatorname{Cov}(X, Y)=E\left[\left(X-\mu_{x}\right)\left(Y-\mu_{y}\right)\right]</script><p>和一维高斯分布类似，在多元高斯分布中 $\mu$ 决定了图像中心的位置，而协方差举证则决定了图像的形状。</p>
<p>为什么要使用多元高斯分布呢？那是因为多元高斯分布有着良好的性质，多元高斯分布对于边缘化和条件运算是封闭的。封闭的意思是，做完这些操作得到的分布仍然是一个多元高斯分布。比如，向量 $X$ 和 $Y$ 服从如下多元高斯分布：</p>
<script type="math/tex; mode=display">
\left[\begin{array}{l}
X \\
Y
\end{array}\right] \sim \mathcal{N}(\mu, \Sigma)=\mathcal{N}\left(\left[\begin{array}{c}
\mu_{X} \\
\mu_{Y}
\end{array}\right],\left[\begin{array}{cc}
\Sigma_{X X} & \Sigma_{X Y} \\
\Sigma_{Y X} & \Sigma_{Y Y}
\end{array}\right]\right)</script><p>首先是边缘化操作，边缘化操作和条件操作分别得到如下分布。</p>
<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1ggyfetk4vlj30nf09kwfh.jpg" alt></p>
<p>了解了必要的公式以后，我们可以看一下如何从视觉层面理解这两个运算。虽然边缘化和条件作用可以用于多维的多元分布，还是用下图中的二维分布作为例子更加好理解。边缘化可以理解为在高斯分布的一个维度上做累加，这也符合边缘分布的一般定义。条件作用也有个很好的几何表达——我们可以把它想象成在多元分布上切下一刀，从而获得一个维数更少的高斯分布。</p>
<p><img src="https://pic1.zhimg.com/80/v2-d6c43ddd21927fa43e846a87531be839_720w.jpg" alt></p>
<p>复习好了多元高斯分布的基础属性，我们接着就可以把它们组装到一起，来定义高斯过程，并展示怎么用高斯过程来解决回归问题。高斯过程的定义就是一组定义在连续域上的随机变量，这组随机变量的子集都服从高斯分布。一个高斯过程由均值函数和协方差函数唯一定义，可以写成如下形式。</p>
<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1ggyh63f3lkj30n109b0ui.jpg" alt></p>
<p>高斯过程是一种非参数化的机器学习工具。他让我们结合先验知识，对数据做出预测，最直观的应用领域就是回归。我们先回顾一下，什么是回归，给定一组训练数据，回归的目的是为了找到一个函数来尽可能的你和这组数据点，这个过程叫做用函数拟合数据。在参数化的方法中，我们需要给定函数模型和损失函数，从而得到一个在给定参数模型下loss值最小的函数。然而，对于一组既定的训练数据，或许潜在有无数个函数可以用来做拟合，高斯函数就是从这个点出发，由训练数据推断函数可能的分布。</p>
<p>可以举一个气温的例子。</p>
<p>下面我们来介绍高斯过程如何做回归。高斯过程回归有两个基本的假设：</p>
<p>一、所有的函数值都来自于一个多元高斯分布，即，给定一组函数值，我们可以毫不犹豫地认为它们服从一个多元高斯分布；</p>
<p>二、另外一个假设是，特征值相近的点其对应的函数值相关性也越高；</p>
<p>这里，我们留下两个疑问，函数值服从怎样的多元高斯分布？即均值向量和协方差矩阵如何得到。另外一个疑问是，如何衡量函数值之间的相似度？不用着急，我们接下来会回答这两个问题。</p>
<p>我们先回答如何衡量函数值之间的相关性，这个问题的答案是核函数（kernel function）。核函数是高斯过程的关键，既然特征值决定了函数值，那么我们就用两个点的特征值来衡量函数值之间的相关性。核函数的主要特性是，两个点的特征越相似，则他们的函数值相关性就越高。核函数的种类非常多，不同的核函数距离计算方法不同，表现出来的特性也不同。最广泛使用的就是高斯核，又称径向基函数，它的表达形式如下：</p>
<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1ggyhrpkv32j30mk02zaa6.jpg" alt></p>
<p>其中，$\sigma$ 和 $l$ 是函数的两个超参数。说到相关性，我们自然会想到协方差矩阵不就是用来衡量相关性的么？没错，核函数就是用来生成协方差矩阵的，核函数的输入是各个点的特征值和函数值无关。</p>
<p>核函数一个比较好的性质在于，我们可以把多个核函数作用到一起，比如我们可以将高斯核和周期核作用到一起，将它们作用叠加的操作就是乘起来。这个技巧在论文中也有使用。</p>
<p>下面，我们正式介绍高斯过程回归。假设我们有一组训练数据点 $(X,Y)$，要得到测试数据 $X^<em>$ 对应的函数值 $Y^</em>$。根据高斯过程的先验，我们可以知道 $Y$ 和 $Y^<em>$ 服从一个 $|Y|+|Y</em>|$ 维的多元高斯分布，这里我们假设均值向量是零向量，这样做是为了运算的方便，实际上我们可以对数据做预处理，先统一减去均值，后面得到预测值再加回来。</p>
<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1ggyi2bu7w0j30lk02paa7.jpg" alt></p>
<p>利用高斯分布的条件运算，我们可以得到 $Y^*$ 的后验分布：</p>
<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1ggyi3jnfm6j30ls0200sq.jpg" alt></p>
<p>有了 $Y<em>$ 的分布，我们直接采样就可以得到具体的 $Y</em>$ 值，并且可以知道值对应的置信度。</p>
<p>下面，我们来总结一下高斯过程方法的优缺点：</p>
<h5 id="优点："><a href="#优点：" class="headerlink" title="优点："></a>优点：</h5><p>优点是可以拟合非线性函数，可以得到预测点值的分布，从而产生多样性的数据以及其置信度；</p>
<h5 id="缺点："><a href="#缺点：" class="headerlink" title="缺点："></a>缺点：</h5><p>缺点也很明显，高斯过程是一个非参数模型，每个点的预测都要对所有的数据点进行运算（矩阵求逆），数据量大时无法适用。</p>
<p>我们想要基于生成的事件样本集合，计算得到 agent 的执勤方案。那么，我们第一步要做的就是生成事件的集合，给定日期和一组区域，我们要能够得到每个区域中事件的数量，以及每个事件对应的优先级，开始时间，处理时间。文中将日期划分为三类：weekday，weekend 和 public holidays，每一类都将训练一个独立的事件预测模型。</p>
<p>事件样本集合的产生流程主要分以下三步：</p>
<p>一、利用 GP 预测事件数量，首先，根据给定的巡逻区域和时间范围，利用学习到的与日期类型对应的高斯过程（GP）模型采样得到每个区域中，事件的数量；</p>
<p>二、将事件映射到时段-区域组合分类，然后，处理单个事件。将历史数据按时间-区域的组合分类，统计每一个类的概率，然后根据概率采样；</p>
<p>三、对于划分到每一个时间-区域类中的事件，我们再根据这个类中历史事件优先级的分布，处理时间的分布，以及所需警员的分布进行采样。事件的开始时间在 2-h 的范围内，以分钟为粒度进行随机。</p>
<h2 id="SAA-Optimization-Model"><a href="#SAA-Optimization-Model" class="headerlink" title="SAA Optimization Model"></a>SAA Optimization Model</h2><p>这里简单介绍下抽样平均近似方法，对于一个随机规划问题 $\min f(x, \xi)$ ，其中 $\xi$ 是一个随机变量，$x$ 是我们需要求解的变量，抽样平均近似的方法的大致含义就是用抽样的方法将随机变量用样本表示，从而将随机规划问题转化为确定性问题。假设随机变量 $\xi$ 的样本分别为：$\xi_1, \xi_2, \dots, \xi_n$，则原问题转化为：$\min \frac{1}{n} \sum_{i=1}^{n} f\left(x, \xi_{i}\right)$。</p>
<p>我们首先介绍一下，模型中涉及的变量：</p>
<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1ggpjsuj8oxj30r209m77f.jpg" alt></p>
<p>下面，我们来看问题的数学模型。</p>
<script type="math/tex; mode=display">
\min \frac{\sum_{r, s} z_{s}^{r}}{\sum_{s}\left|\mathcal{R}_{s}\right|} \tag{1}</script><p>首先，我们看到问题的优化目标，分母是所有样本集合中事件数量的总和。变量 $z_s^r$ 是一个 0-1 变量，取值为 1 时，表明样本集合 $s$ 中的事件 $r$ 响应失败，反之为响应成功。因此，分子上是响应失败事件的总数量，因此优化目标最终是要最小化响应失败率。</p>
<p>下面，我们看到模型的约束。原文中约束的顺序比较混乱，我们下面讲述时，尽可能的把涉及到相同变量的约束放到一起讲。我们先看一组和决策变量 $y$ 相关的约束。</p>
<script type="math/tex; mode=display">
y_{i}=y_{i, s} \quad \forall i, s \tag{2}</script><script type="math/tex; mode=display">
\sum_{i} y_{i, s}=Y_{\max } \quad \forall s \tag{3}</script><script type="math/tex; mode=display">
y_{i, s}^{r} \leq y_{i, s} \quad \forall i, r, s \tag{11}</script><script type="math/tex; mode=display">
\sum_{i} y_{i, s}^{r}=d_{s}^{r} \quad \forall r, s \tag{12}</script><script type="math/tex; mode=display">
y_{i, s}^{q, r} \leq y_{i, s}^{q} \quad \forall i, q \leq r, s \tag{13}</script><script type="math/tex; mode=display">
y_{i, s}^{q, r} \leq y_{i, s}^{r} \quad \forall i, q \leq r, s \tag{14}</script><script type="math/tex; mode=display">
\sum_{q} y_{i, s}^{q, r}=y_{i, s}^{r} \quad \forall i, q \leq r, s \tag{15}</script><script type="math/tex; mode=display">
\sum_{r} y_{i, s}^{q, r} \leq 1 \quad \forall q<r, s \tag{16}</script><script type="math/tex; mode=display">
\sum_{q} y_{i, s}^{q, r} \geq \sum_{h} y_{i, s}^{r, h} \quad \forall i, q<r<h, s \tag{17}</script><script type="math/tex; mode=display">
y_{i, s}^{q, r}=0, \quad \forall i, q \geq r, s \tag{18}</script><script type="math/tex; mode=display">
y_{i,s}^{q, r} \leq \frac{t_s^{r}-t_s^{q}}{M} \quad \forall i, q, r, s \tag{19}</script><p>约束（2）中，$y_i$ 表示 agent $i$ 是否被激活，$y_{i,s}$ 表示 agent $i$ 在样本 $s$ 中有没有被激活。这个约束主要保障了激活状态的一致性，即在一个样本中激活，在所有样本中都被激活。</p>
<p>约束（3）主要限定，在每个样本中激活 agent 的总数要和设定的总的可用人数 $Y_{max}$ 相一致。</p>
<p>约束（11）限定，当 agent $i$ 没有激活时，不可能响应任何事件。</p>
<p>约束（12）主要限定样本集 $s$ 中事件 $r$ 的响应人数要符合其要求，$d_s^r$ 对应响应人数。</p>
<p>约束（13）保障当 agent $i$ 没有响应 $q$ 时，也不可能出现响应 $q$ 后响应 $r$ 的情况。</p>
<p>约束（14）保障当 agent $i$ 没有响应 $r$ 时，不可能出现先响应 $q$ 后响应 $r$ 的情况。</p>
<p>约束（15）限制了两种情况，一、事件 $r$ 没有被 agent $i$ 所响应，那么对于 $i$ 而言，$r$ 不可能成为任何一个在它之前发生事件的后继响应事件；二、事件 $r$ 被 agent $i$ 所响应，那么，有且仅有一个在 $r$ 之前发生的事件是 $r$ 的前驱事件。</p>
<p>约束（16）限定，对于一个 agent $i$ 而言，事件 $q$ 的后继事件最多不超过一，即在 $q$ 之后被 $i$ 响应事件的数量不超过1。</p>
<p>约束（17）的解释是：对于 agent $i$ 和事件 $r$ 而言，事件 $r$ 如果被 $i$ 所响应，那么 $r$ 必有前驱事件，但不一定有后继事件。</p>
<p>约束（18）限定，对于 agent $i$ 而言，他不可能先响应后出现的事件。</p>
<p>约束（19）我感觉有一点问题，没看懂它要限制什么，我们略过。</p>
<p>下面，我们再看一组和事件结束时间相关的约束。</p>
<script type="math/tex; mode=display">
e_{s}^{r} \leq \delta_{s}^{r}+T_{l^{r}, l^{i}}+g_{s}^{r}+M\left(1-y_{i, s}^{r}\right) \quad \forall i, r, s \tag{20}</script><script type="math/tex; mode=display">
e_{s}^{r} \geq \delta_{s}^{r}+T_{l^{r}, l^{i}}+g_{s}^{r}+M\left(y_{i, s}^{r}-1\right) \quad \forall i, r, s \tag{21}</script><p>约束（20）和（21）之间也存在问题，当事件 $r$ 只被一个 agent 响应时，不存在问题，$e_s^r$ 在大于等于和小于等于的限定下，只能等于 $\delta_{s}^{r}+T_{l^{r}, l^{i}}+g_{s}$。但是当超过一个 agent 响应时，以上两个约束之间就会产生矛盾。由于 $e_s^r$ 是一个针对所有 agent 共享的变量，因此我们认为更实际的设定是响应的 $r$ 的多个 agent 都返回驻地后，这些 agent 才能开始响应其他事件，即 $e_s^r = \max \{\delta_s^r + g_s^r + T_{l^r,l^i} + M(y_{i,s}-1)\}$。 下面，我们尝试按照以上思路修改一下约束 (20) 和（21）。我们需要引入一个新的辅助变量 $c_{i,s}^r$，这是一个 0-1 变量。</p>
<script type="math/tex; mode=display">
\begin{array}
&e_{i,s}^r = \delta_s^r + T_{l^r, l^i} + g_s^r + M \times (y_{i,s}^r - 1) \quad \forall i,r,s\\
e_s^r \ge e_{i,s}^r \quad \forall r,s,i \\
e_s^r \le e_{i,s}^r - M \times c_{i,s}^r \quad \forall r,s,i \\
\sum_{i=1}^{\vert \mathcal{I} \vert} c_{i,s}^r = \vert \mathcal{I} \vert - 1 \quad \forall r,s \\
c_{i,s}^r \in \{0,1\} \quad \forall i,r,s
\end{array}</script><p>下面，我们看一组和变量 $\Delta_s^{q,r}$ 相关的约束，上面我们介绍过 $\Delta_s^{q,r}$ 反映了事件 $r$ 和事件 $q$ 的周期是否重叠。如果事件 $q$ 的结束时间早于事件 $r$ 的开始时间，那么两者不重叠，否则重叠，重叠为1，不重叠为0，这一限制通过约束 （6）和（7）可以实现。</p>
<script type="math/tex; mode=display">
\Delta_{s}^{q, r} \geq 0 \quad \forall q<r, s \tag{6}</script><script type="math/tex; mode=display">
1+\frac{e_{s}^{q}-t_{s}^{r}}{M} \geq \Delta_{s}^{q, r} \geq \frac{e_{s}^{q}-t_{s}^{r}}{M} \quad \forall q<r, s \tag{7}</script><p>下面，我们看和时间的处理结束时间 $\delta_s^r$ 相关的约束，再次强调这里的事件处理结束时间实际上是参与处理该事件的 agent 可以再次响应其他事件的最早时间。我们先看第一组：</p>
<script type="math/tex; mode=display">
T_{l^{i}, l^{r}}+t_{s}^{r}+M \cdot\left(y_{i, s}^{r}-1\right) \leq \delta_{s}^{r} \quad \forall r, i, s \tag{4}</script><script type="math/tex; mode=display">
T_{l^{i}, l^{r}}+e_{s}^{q}+M \cdot\left(y_{i, s}^{q, r}-1\right) \leq \delta_{s}^{r} \quad \forall i, q<r, s \tag{5}</script><script type="math/tex; mode=display">
T_{l^{i}, l^{r}}+t_{s}^{r}+M \cdot\left(1-y_{i, s}^{q, r}\right) \geq \delta_{s}^{r} \quad \forall q=0, r, i, s \tag{8}</script><p>约束（4），主要是为了限制当有多个 agent 被派遣处理同一个事件时，事件 $r$ 的响应时间 $\delta_s^t$ 为第一个到达现场的 agent 的到达时间。我们首先看到约束（4），对于多 agent 响应同一个事件的情况，约束（4）同样是存在问题的。我们假设 agent $i_1$ 和 $i_2$ 都需要响应事件 $r$，并且此时 $i_1$ 和 $i_2$ 都处在空闲状态，同时我们假设 $i_1$ 距离 $r$ 的位置更近，因此事件 $r$ 的响应时间 $\delta_s^r$ 应该等于 $T_{l^{i_1},l^{r}} + t_s^r$，这个值应当是小于等于 $T_{l^{i_2},l^r} + t_s^r$的，而在约束（4）中 ，$\delta_s^r$ 是要大于等于 $T_{l^{i_2},l^r} + t_s^r$ 的这就产生了矛盾。实际上约束（4）的原意是要保障，$\delta_s^r$ 大于等于理想状态下的最小值。什么是理想状态的下的最小值？我们首先关心需要响应事件 $r$ 的 agent 的集合，我们假设在这个集合中所有 agent 在 $r$ 产生时都处于空闲状态，那么都立即从驻地赶往 $r$ 的发生地，那么响应时间自然是 $T_{l^i,l^r} + t_s^r$ 中的最小值。当响应的 agent 存在都处在非空闲状态的可能，那么 $\delta_s^r$ 自然要大于等于前面的极小值。所以，我们用约束（4.1）代替约束（4），在约束（4.1）中我们增加了几个辅助变量 $\alpha_{i,s}^r$，$\beta_{s}^r$ 和 $h_{i,s}^r$，其中 $\alpha_{i,s}^r$ 表示 agent $i$ 响应事件 $r$ 时的最早时间，这个可以分两种情况看，一、$y_{i,s}^r = 0$，即 agent $i$ 不响应事件 $r$ ，由于 M 是一个很大的数，因此 $\alpha_s^r$ 可以近似为 M，二、$y_{i,s}^r = 1$，即 agent $i$ 需要响应事件 $r$，则其理论上的最早响应时间 $\alpha_{i,s}^r$ 为$T_{l^i,l^r} + t_s^r$。$\beta_s^r$ 表示的是事件 $r$ 最小的理论最早响应时间，即 $\min \{\alpha_{i,s}^r\}$，约束（4.1）中约束 2~4 便是为了得到 $\beta_s^r$，首先，如第四个公式所示，$\beta_s^r$ 小于等于所有的 $\alpha_{i,s}^r$。同时， 我们引入一个新的 0-1 变量 $h_{i,s}^r$，对每个事件来说，与其相关的 $\vert \mathcal{I} \vert$ 个 h 变量的和为 $\vert \mathcal{I} \vert - 1$，这个为了使得（4.1）中的 $\vert \mathcal{I} \vert$ 个（2）不等式只有一个被激活，因为 $h=1$ 时不等式显然成立，当 h=0 时，要求 $\beta_s^r \ge \alpha_{i,s}^r$，结合上面 $\beta_r^s \le \alpha_{i,s}^r$ 的限制，当且仅当 $\beta_r^s = \min{\alpha_{i,s}^r}$ 时所有不等式成立。接下来我们只要让 $\delta_s^r \ge \beta_s^r$ 就达到了我们的目的。</p>
<script type="math/tex; mode=display">
\begin{array}
&T_{l^{i}, l^{r}}+t_{s}^{r}+M \cdot\left(1 - y_{i, s}^{r}\right) = \alpha_{i,s}^r \quad \forall r, i, s \\
\beta_{s}^r \ge \alpha_{i,s}^{r} - M \times h_{i,s}^r \quad \forall r,i,s \\
\beta_{s}^r \le \alpha_{i,s}^{r} \quad \forall r,i,s \\
\sum_{i=1}^{\vert \mathcal{I} \vert} h_{i,s}^r = \vert \mathcal{I} \vert -1 \quad \forall r,s \\
h_{i,s}^r \in \{0,1\} \quad \forall r,i,s \\
\delta_{s}^r \ge \beta_s^r \quad \forall r,s \\
\end{array}\tag{4.1}</script><p>同样的问题也存在于约束（5）中，约束 (5) 描述的是 agent $i$ 响应事件 $r$ 时正在处理其他事件的情况，在这种情况下，agent $i$ 必须先完成手头的工作 $q$，再去处理 $r$，因此最早的响应时间为 $T_{l^i,l^r} + e_s^q$。$y_{i,s}^{q,r} = 1$ 表示 agent $i$ 在响应 $q$ 之后响应 $r$，$e_s^q$ 表示 $i$ 响应完 $q$ 回到驻地的时间，那么对于所有 $y_{i,s}^{q,r} = 1$ 的 agent，$\delta_s^r$ 应该大于等于他们 $T_{l^i,l^r} + e_s^q$ 中的最小值。这可以参照约束 (4) 进行相对应的修改，此处不再赘述。</p>
<p>约束（8）中，当 $q=0$ 时，如果 $y_{i,s}^{q,r} = 1$，那么事件 $r$ 实际上是 agent $i$ 响应的第一个事件，那么事件 $r$ 的响应时间确实会小于等于 $T_{l^i,l^r} + t_s^r$。</p>
<p>下面，我们看到另一组约束（9），（10），（23）和（24）。</p>
<script type="math/tex; mode=display">
T_{l^{i}, l^{r}}+t_{s}^{r} +M \cdot \Delta_{s}^{q, r} +M \cdot\left(1-y_{i, s}^{q, r}\right) \geq \delta_{s}^{r} \quad \forall q<r, i, s \tag{9}</script><script type="math/tex; mode=display">
T_{l^{i}, l^{r}}+e_{s}^{q} +M \cdot\left(1-\Delta_{s}^{q, r}\right) +M \cdot\left(1-y_{i, s}^{q, r}\right) \geq \delta_{s}^{r} \quad \forall q<r, i, s \tag{10}</script><script type="math/tex; mode=display">
T_{l^{i}, l^{r}}+ t_{s}^{r}+M \cdot \Delta_{s}^{q, r} +M \cdot\left(1-y_{i, s}^{q, h}\right) \geq \delta_{s}^{r} \quad \forall i, q<r<h, s \tag{23}</script><script type="math/tex; mode=display">
T_{l^{i}, l^{r}}+ e_{s}^{q}+M \cdot\left(1-\Delta_{s}^{q, r}\right) +M \cdot\left(1-y_{i, s}^{q, h}\right) \geq \delta_{s}^{r} \quad \forall i, q<r<h, s \tag{24}</script><p>上文我们说过 $\Delta_s^{q,r}$ 是事件 $r$ 和事件 $q$ 是否重叠的标志，当两者不重叠且连续被同一 agent 执行时，执行后者时，agent 是从空闲状态开始的。下面我们分四种情况讨论约束（9）。1）当 $\Delta_s^{q,r} = 1$ ，$y_{i,s}^{q,r}=0$ 时，约束（9）显然成立；2）当 $ \Delta_s^{q,r} = 0$，$y_{i,s}^{q,r} = 0$ 时，约束（9）显然成立；3）当 $ \Delta_s^{q,r}$ 和 $y_{i,s}^{q,r}$ 取值都为 1 时，约束（9）也显然成立；4）当 $ \Delta_s^{q,r}$ 为0，且 $y_{i,s}^{q,r}$ 为 1 时，由于 $r$ 发生时，$q$ 已经结束，因此 agent $i$ 是从空闲状态开始响应 $r$ 的，很显然 $r$ 的响应时间肯定不晚于 $T_{l^{i}, l^{r}}+t_{s}^{r}$。</p>
<p>我们再看约束（10），同样分四种情况，前三种情况 1）$\Delta_s^{q,r} = 0$，$y_{i,s}^{q,r} = 0$ ，2）$\Delta_s^{q,r} = 1$，$y_{i,s}^{q,r} = 0$ ；3）$\Delta_s^{q,r} = 0$，$y_{i,s}^{q,r} = $ 1 时，约束（10）都显然成立；我们重点看第四种情况，即 $\Delta_s^{q,r} = 1$，$y_{i,s}^{q,r} = 1$ ，这时约束（10）可以改写为： </p>
<script type="math/tex; mode=display">
T_{l^{i}, l^{r}}+e_{s}^{q}  \ge \delta_s^r \quad \forall q < r, i, s \tag{10.1}</script><p>由于 $\Delta_s^r = 1$，因此当 $r$ 发生时，$q$ 还没有处理完， agent $i$ 是在 $q$ 完成后立即前往 $r$ 的，因此 $r$ 的响应时间肯定不会晚于 $i$ 到达现场的时间，即 $T_{l^{i}, l^{r}}+e_{s}^{q}$。</p>
<p>有了（9）和（10）的基础，我们接着看（23）和（24）。对于约束（23），当 $\Delta_s^{q,r} = 1$ 时，无论 $y_{i, s}^{q, h}$  的取值是多少，不等式成立，当 $\Delta_s^{q,r}$，$y_{i, s}^{q, h}$ 都为0时，也显然成立。我们重点需要分析的是 $\Delta_s^{q,r} = 0$，$y_{i, s}^{q, h} = 1$ 的情况，我们将约束（23）改写为（23.1）：</p>
<script type="math/tex; mode=display">
T_{l^{i}, l^{r}}+ t_{s}^{r} \geq \delta_{s}^{r} \quad \forall i, q<r<h, s \tag{23.1}</script><p>$y_{i,s}^{g,h} = 1$ 说明 agent $i$ 在处理完 $q$ 后紧接着处理了 $h$，从时间上看 $r$ 的出现比 $h$ 早，那为什么 $i$ 没有响应 $r$ 而响应 $h$ 呢？根据问题中设定的原则，最近可使用的 $d_s^r$ 个 agents 响应的原则，那是因为有 agent 比 $i$ 更早地到达 $r$ 的现场，满足了 $r$ 的处理人数要求。又因为 $\Delta_s^{q,r}=0$，因此任务 $q$ 和任务 $r$ 不重叠，agent $i$ 可以从空闲状态下响应 $r$。综合以上两点，事件 $r$ 的响应时间应该不晚于 $i$ 从空闲状态到达 $r$ 的现场的时间，即 $T_{l^i.l^r} + t_s^r$，因此（23.1）也是成立的。</p>
<p>同样的，对于约束（24），我们重点分析  $\Delta_s^{q,r} = 1$，$y_{i, s}^{q, h} = 1$ 的情况，这时约束可以改写为：</p>
<script type="math/tex; mode=display">
T_{l^{i}, l^{r}}+ e_{s}^{q} \geq \delta_{s}^{r} \quad \forall i, q<r<h, s \tag{24.1}</script><p>在这种情况下，我们同样需要分析 $i$ 没有响应 $r$ 的原因。根据 $\Delta_s^{q,r} = 1$ ，我们知道 $q$ 和 $r$ 是重叠的，即 $r$ 出现时，$q$ 还没有结束。因此，如果 $i$ 响应 $r$ 就必须先完成 $q$，这样他赶到 $r$ 发生地的时间是 $T_{l^{i}, l^{r}}+ e_{s}^{q}$，由于 $r$ 实际上没有要 $i$ 响应，因此 $r$ 的响应时间应该不晚于 $T_{l^{i}, l^{r}}+ e_{s}^{q}$，即约束（23.1）是成立的。</p>
<p>下面，我们看到最后一组比较复杂的约束。这一组约束也是为了保障“Greedy”响应的。</p>
<script type="math/tex; mode=display">
T_{l^{i}, l^{r}}+t_{s}^{r}+M \cdot \sum_{h=1}^{r-1} y_{i, s}^{q=0, h} \geq \delta_{s}^{r} \quad \forall i, r, s \tag{25}</script><script type="math/tex; mode=display">
T_{l^{i}, l^{r}}+t_{s}^{r} +M \cdot \Delta_{s}^{q, r} +M \cdot \sum_{h=q+1}^{r-1} y_{i, s}^{q, h} \geq \delta_{s}^{r} \quad \forall i, q<r, s \tag{26}</script><script type="math/tex; mode=display">
T_{l^{i}, l^{r}}+e_{s}^{q} +M \cdot\left(1-\Delta_{s}^{q, r}\right) +M \cdot \sum_{h=q+1}^{r-1} y_{i, s}^{q, h} \geq \delta_{s}^{r} \quad \forall i, q<r, s \tag{27}</script><p>我们首先看到约束（25），如果在 $r$ 之前 agent $i$ 已经响应过其他其他事件的话，不等式恒成立。反之，$i$ 处于空闲状态，那么 $r$ 的响应时间不会晚于 $i$ 从驻地出发到达 $r$ 发生地的时间，即 $T_{l^i,l^r} + t_s^r$。</p>
<p>同样地，在约束（26）中，如果事件 $q$ 和 $r$ 不重叠，且 agent $i$ 在事件 $q$ 和 $r$ 之间没有响应其他事件的话，那么事件 $r$ 的响应时间不会晚于 $i$ 从驻地到达 $r$ 发生地的时间，即 $T_{l^i,l^r} + t_s^r$。</p>
<p>约束（27）和（26）相对应，如果事件 $q$ 和 $r$ 重叠，且 agent $i$ 在事件 $q$ 和 $r$ 之间没有响应其他事件的话，那么事件 $r$ 的响应时间不会晚于 $i$ 在完成事件 $q$ 后出发到达 $r$ 发生地的时间，即 $T_{l^i,l^r} + e_s^q$。</p>
<p>最后一个约束（22）和问题的优化目标相关。</p>
<script type="math/tex; mode=display">
z_{s}^{r} \geq \frac{\left(\delta_{s}^{r}-t_{s}^{r}\right)-T_{c}}{M} \quad \forall r, c^{r}=c, s \tag{22}</script><p>约束中 $T_c$ 是 $r$ 的 QoS 目标值，$\delta_s^r - t_s^r$ 是 $r$ 的反应时间，如果这个时间不超过 $r$ 的 QoS 目标，那么 $z_s^r$ 可以取 0，反之只能取1。</p>
<p>好了，以上就是问题的 MIP 模型。</p>
<h2 id="Iterated-Local-Search-Heuristic"><a href="#Iterated-Local-Search-Heuristic" class="headerlink" title="Iterated Local Search Heuristic"></a>Iterated Local Search Heuristic</h2><p>使用 SAA 方法时，问题的解空间会随着样本数量的增加快速扩大，为了能够解决大规模问题，本文提出了一种基于迭代区域启发式搜索的算法。该算法主要包括三部分：</p>
<ol>
<li>贪心的产生初始解，每一步激活一个 agent，只要 agent 加入区域可以降低响应失败率，就将 agent 安排进这个区域，直到 $Y_{max}$ 个 agents 都分配完毕；</li>
<li>局部搜索。按照一定顺序，每次改变一个 agent 的位置，如果可以导致更低的响应失败率就保存下来。直到迭代不能产生更好的解，就停止；</li>
<li>和当前最优解比较，如果更好，就替代当前最优解。对最优解进行扰动（perturbation operation），随机更换  $p$ 个 agent 的位置，产生新的解，进行局部搜索；</li>
<li>当 2-3 步的迭代到达预先设定的轮次 $T_{max}$ 后，返回当前找到的最优解；</li>
</ol>
<h2 id="Numerical-Results"><a href="#Numerical-Results" class="headerlink" title="Numerical Results"></a>Numerical Results</h2><p>实验环境的设定</p>
<ol>
<li>使用一年的数据训练，测试事件预测模型；</li>
<li>使用 CPLEX solver 求解 MIP 问题，截止时间为 24 h；</li>
<li>巡逻区域包含 24 个区域；</li>
<li>巡逻时长为 24 小时；</li>
<li>紧急和非紧急事件的响应时间需求分别为 15 和 30 分钟；</li>
</ol>
<p>实验的结果：</p>
<ol>
<li>当 agent 数量为7， sample 数量为 1 时，MIP 求解时间为 84.05 s；</li>
<li>当 agent 数量为7，sample 数量为 3 时，MIP 求解时间超过了 24 h；</li>
<li>当 agent 数量为 10 ，sample 数量为 1 时，MIP 求解时间超过了 12 h；</li>
</ol>
<p>生成方案的对比</p>
<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1ggrz1juntjj30hv088mzh.jpg" alt></p>
<p>预测模块的准确性</p>
<p>利用 17 个周的历史数据，进行训练。蓝色基于模型预测，红色基于准确数据。利用一周的历史数据训练事件生成器，然后生成数据。利用 ILS 生成部署计划，在17个星期上做了评估，两者的差距不超过5%。</p>
<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1ggrybt1911j30hi08kmzr.jpg" alt></p>
<p>运行时间和解质量之间的权衡</p>
<p>根据不同的 sample 数量和 agent 数量组合计算巡逻方案。大概 25 个 sample 就足以衡量一周事件的动态性，对于 24 h 的巡逻周期，计算 15~20 min 比较合适。</p>
<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1ggryq8q2qyj30hl03m0ta.jpg" alt></p>
<h2 id="建模技巧"><a href="#建模技巧" class="headerlink" title="建模技巧"></a>建模技巧</h2><p>有一些问题中，尽管还有特殊的约束或者特殊变量，看上去不是线性规划问题，但我们仍然可以通过一些建模技巧，将它们转化为线性规划。以下内容参考了文献【7】。</p>
<h3 id="一、约束中包含最小值"><a href="#一、约束中包含最小值" class="headerlink" title="一、约束中包含最小值"></a>一、约束中包含最小值</h3><p>比如，有以下规划问题：</p>
<script type="math/tex; mode=display">
\min z \\
s.t. \quad z = \min \{x_1,x_2,x_3\}</script><p>为了将原问题中的约束转化为线性约束，我们引入一组 0-1 变量 $y_1,y_2,y_3$ 和一个充分大的数 $M$，将原问题的约束改写为如下约束：</p>
<script type="math/tex; mode=display">
\min z \\
s.t. \quad z \ge x_1 - M \times y_1 \\
z \ge x_2 - M \times y_2 \\
z \ge x_3 - M \times y_3 \\
\sum_{i=1}^3 y_i = 3 - 1 = 2</script><h3 id="二、含有绝对值的建模"><a href="#二、含有绝对值的建模" class="headerlink" title="二、含有绝对值的建模"></a>二、含有绝对值的建模</h3><p>比如，有以下规划问题：</p>
<script type="math/tex; mode=display">
\min \|X\|_{1}=\sum_{i=1}^{n}\left|x_{i}\right| \\

s.t. \quad AX \le b</script><p>为了将原问题转化为线性规划问题，引入两个新的非负向量 $u$ 和 $v$ ，满足以下条件：</p>
<script type="math/tex; mode=display">
\left\{\begin{array}{l}
\left|x_{i}\right|=u_{i}+v_{i} \\
x_{i}=u_{i}-v_{i} \\
u_{i} \geq 0, v_{j} \geq 0
\end{array}\right.</script><p>上述规划问题可以转化为如下线性规划问题：</p>
<script type="math/tex; mode=display">
\begin{array}{l}
\min \sum_{i=1}^{n}\left(u_{i}+v_{i}\right) \\
\text {s.t.}\left\{\begin{array}{l}
A(u-v) \leq b \\
u \geq 0, v>0
\end{array}\right.
\end{array}</script><h3 id="三、含有最大（最小）值的建模"><a href="#三、含有最大（最小）值的建模" class="headerlink" title="三、含有最大（最小）值的建模"></a>三、含有最大（最小）值的建模</h3><p>比如，有以下规划问题：</p>
<script type="math/tex; mode=display">
\min _{X} \max _{Y}(a X+b Y)</script><p>为了将原问题转化为线性规划问题，引入新的变量 $u$，满足以下条件：</p>
<script type="math/tex; mode=display">
u=\max _{Y}(a X+b Y) \Longleftrightarrow a X+b Y \le u</script><p>上述规划问题就转化为如下线性规划问题：</p>
<script type="math/tex; mode=display">
\begin{aligned}
&\min \quad u\\
&\text {s.t. } \quad a X+b Y \leq u
\end{aligned}</script><h3 id="四、二选一的约束建模"><a href="#四、二选一的约束建模" class="headerlink" title="四、二选一的约束建模"></a>四、二选一的约束建模</h3><p>比如，有以下约束：</p>
<script type="math/tex; mode=display">
2 x+3 y \leqslant 100 \text { or } x+y \le 50</script><p>为将其转换为线性约束，引入一个新的 0-1 变量 $z$ 和一个充分大的数 $M$。</p>
<script type="math/tex; mode=display">
\begin{array}{l}
2 x+3 y \leq 100 \text { or } x+y \leq 50 \\
\Longleftrightarrow 
\left\{\begin{array}
\quad2 x+3 y \leq 100+z M \\
x+y \leq 50+(1-z) M \\
z \in\{0,1\}
\end{array}\right.
\end{array}</script><h3 id="五、多选多约束建模"><a href="#五、多选多约束建模" class="headerlink" title="五、多选多约束建模"></a>五、多选多约束建模</h3><p>在第一个求 min 的例子中，我们实际上已经利用到了这个技巧。比如，有以下三个约束，我们想要满足其中两个：</p>
<script type="math/tex; mode=display">
2 x+3 y \leq 100 \text{ or } x+y \leq 50 \text{ or } x+2 y \le 80</script><p>此处，我们还是使用大 M 法，引入三个0-1变量和一个充分大的数 $M$。</p>
<script type="math/tex; mode=display">
\begin{array}{c}
2 x+3 y \leq 100 \text { or } x+y \leq 50 \text { or } x+2 y \leq 80 \\
\qquad \begin{array}{l}
2 x+3 y \leq 100+z_{1} \times M \\
x+y \leq 50+z_{2} \times M \\
x+2 y \leq 80+z_{3} \times M \\
z_{1}+z_{2}+z_{3}=3-2 \\
z_{1}, z_{2}, z_{3} \in\{0,1\}
\end{array}
\end{array}</script><h3 id="六、固定成本约束的建模"><a href="#六、固定成本约束的建模" class="headerlink" title="六、固定成本约束的建模"></a>六、固定成本约束的建模</h3><p>在库存问题中，通常考虑订货的固定成本和可变成本。就是说，只要订货 $x&gt;0$，就有一个固定成本 $k$，和可变成本 $cx$，它的成本函数就是：</p>
<script type="math/tex; mode=display">
z(x)=\left\{\begin{array}{cc}
0, & x=0 \\
c x+k, & x>0
\end{array}\right.</script><p>这实际上也是一个二选一的约束，为了将其转换为线性约束。引入一个0-1变量 $y$ 和一个充分大的数 $M$。</p>
<script type="math/tex; mode=display">
\begin{aligned}
 &z(x)=c x+k y \\
\text { s.t. } &\quad x \leq y \times M \\
&x + (1-y) \times M > 0 \\
&y \in \{0,1\} \\
&x \ge 0
\end{aligned}</script><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>本文编写参考了以下资料，如果有看不明白的地方，可以点击链接查看资料原文。</p>
<ol>
<li><a href="https://www.ijcai.org/Proceedings/2019/0806.pdf" target="_blank" rel="noopener">Improving Law Enforcement Daily Deployment Through Machine Learning-Informed Optimization under Uncertainty, IJCAI 2019</a></li>
<li><a href="[http://idataskys.com/2019/05/01/%E9%AB%98%E6%96%AF%E8%BF%87(Gaussian%20Process">高斯过程的理解</a>%E7%A8%8B%E7%90%86%E8%A7%A3/](<a href="http://idataskys.com/2019/05/01/高斯过(Gaussian" target="_blank" rel="noopener">http://idataskys.com/2019/05/01/高斯过(Gaussian</a> Process)程理解/)</li>
<li><a href="http://kzyjc.cnjournals.com/ch/reader/create_pdf.aspx?file_no=2012-1486&amp;flag=1&amp;year_id=2013&amp;quarter_id=8" target="_blank" rel="noopener">高斯过程回归方法综述</a></li>
<li><a href="https://mp.weixin.qq.com/s/0NODzd07iEEtjyH3VmmEaQ" target="_blank" rel="noopener">看得见的高斯过程</a></li>
<li><a href="https://people.orie.cornell.edu/shane/pubs/SAAGuide.pdf" target="_blank" rel="noopener">A Guide to Sample-Average Approximation</a></li>
<li><a href="cs.cornell.edu/courses/cs4780/2018fa/lectures/lecturenote15.html">Lecture 15: Gaussian Processes</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/69397833" target="_blank" rel="noopener">优化 | 线性规划和整数规划的若干建模技巧</a></li>
</ol>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  


          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">COOLA-LAB</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/%7C%7C%20archive">
              
                  <span class="site-state-item-count">10</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">1</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">19</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">COOLA-LAB</span>

  
</div>

<!--

  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>




  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Mist</a> v5.1.4</div>



-->

        
<div class="busuanzi-count">
  <script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  
    <span class="site-uv">
      <i class="fa fa-user"></i>
      <span class="busuanzi-value" id="busuanzi_value_site_uv"></span>
      
    </span>
  

  
    <span class="site-pv">
      <i class="fa fa-eye"></i>
      <span class="busuanzi-value" id="busuanzi_value_site_pv"></span>
      
    </span>
  
</div>








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
